{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyN8CHaZiYZ4",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdf5c1e89cd73689",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lab 2: Sequence Models\n",
    "\n",
    "In this lab, you will implement a **Hidden Markov Model (HMM)**. HMMs specify a **joint** probability over **observations** and **hidden states**. \n",
    "\n",
    "This is what we will do today:\n",
    "\n",
    "1. **Estimate** a simple HMM model from training data (supervised learning)\n",
    "2. Find the *best* sequence of hidden states for a given sequence of observations (we define \"best\" in 2 different ways!)\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "### Rules\n",
    "* The lab exercises should be made in **groups of two people**.\n",
    "\n",
    "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**. See Blackboard for date of submission. \n",
    "\n",
    "* The **filename** should be `lab2_id1_id2.ipynb`.\n",
    "\n",
    "* The notebook is graded on a scale of **0-50**. The number of points for each question is indicated in parantheses. (Note: The total of 50 points is only for convenience and will be scaled- each of the four labs has the same weight in the course.) \n",
    "\n",
    "* The questions marked **Extra** are not obligatory; try them if you want an extra challenge. \n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should **write your code and answers in this iPython Notebook** (see http://ipython.org/notebook.html for reference material). If you have problems, please contact your teaching assistant.\n",
    "\n",
    "* Use only **one cell for code** and **one cell for markdown** answers!    \n",
    "\n",
    "    * Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
    "    \n",
    "    * For theoretical question, put your solution in the YOUR ANSWER HERE cell.\n",
    "    \n",
    "* Test your code and **make sure we can run your notebook**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYtiw-uliYZ9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c20720f24702422e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "$ \\Sigma := \\{ o_1, \\dots, o_J \\} $ is our set of **observations**\n",
    "\n",
    "$\\Lambda := \\{c_1, \\dots, c_K \\}$ is our set of **state labels**\n",
    "\n",
    "$\\Sigma^*$ are **all possible sequences** of observations (including empty string $\\epsilon$)\n",
    "\n",
    "$\\Lambda^*$ all possible sequences of hidden states (including empty string $\\epsilon$)\n",
    "\n",
    "> Extra info: we can say that $\\Sigma^*$ is the [Kleene-closure](https://en.wikipedia.org/wiki/Kleene_star) of $\\Sigma$, and $\\Lambda^*$ the Kleene-closure of $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU2wjdGviYZ_",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0563c61a6ff0ee4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## A simple example: The Baby HMM\n",
    "\n",
    "We start with a simple example, so that we can easily verify that our code is correct.\n",
    "\n",
    "Consider that we are modeling how a baby behaves. We observe the baby doing the following things: laughing (`laugh`), crying (`cry`), and sleeping (`sleep`). This is our set $\\Sigma$ of **observations**. \n",
    "\n",
    "We presume that, at any moment, the baby can be either `hungry`, `bored`, or `happy`. Since babies cannot talk, each of these states is hidden. This is our set $\\Lambda$ of **hidden states**.\n",
    "\n",
    "**Now the question is: if we have a series of observations, can we predict what hidden states the baby went through?**\n",
    "\n",
    "To tackle this problem, we assume that the baby behaves like a **1st order discrete Markov chain**: the baby's current state only depends on its previous hidden state. The baby can be described as an HMM. (Yay!)\n",
    "\n",
    "For example, assume we observed the baby doing the following:\n",
    "\n",
    "```\n",
    "sleep cry laugh cry\n",
    "cry cry laugh sleep\n",
    "```\n",
    "\n",
    "We will use these sequences as our **test set**. We can try to find out the states of the baby for those observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmRqiS7WiYaB",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2fe112701eae5d42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, to train our model, we will need some examples of **observations** and **states**; this is our **training set**:\n",
    "\n",
    "```\n",
    "laugh/happy cry/bored cry/hungry sleep/happy\n",
    "cry/bored laugh/happy cry/happy sleep/bored\n",
    "cry/hungry cry/bored sleep/happy\n",
    "```\n",
    "\n",
    "So we have **pairs** observation/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "j20Sz9ZWiYaD",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a80c3b9c59cfc01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2RKaP-v_iYaE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab15ee4ee723c00e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set (observations):\n",
      "['sleep', 'cry', 'laugh', 'cry']\n",
      "['cry', 'cry', 'laugh', 'sleep']\n",
      "\n",
      "training set (observation/state pairs):\n",
      "[laugh/happy, cry/bored, cry/hungry, sleep/happy]\n",
      "[cry/bored, laugh/happy, cry/happy, sleep/bored]\n",
      "[cry/hungry, cry/bored, sleep/happy]\n"
     ]
    }
   ],
   "source": [
    "# read in test data\n",
    "test_data = \"\"\"sleep cry laugh cry\n",
    "cry cry laugh sleep\"\"\"\n",
    "\n",
    "def test_reader(test_lines):\n",
    "    for line in test_lines.splitlines():\n",
    "        yield line.split()\n",
    "\n",
    "test_set = list(test_reader(test_data))\n",
    "\n",
    "# read in train data\n",
    "train_data = \"\"\"laugh/happy cry/bored cry/hungry sleep/happy\n",
    "cry/bored laugh/happy cry/happy sleep/bored\n",
    "cry/hungry cry/bored sleep/happy\"\"\"\n",
    "\n",
    "# for convenience, we define a Observation-State pair class\n",
    "# fyi, more about namedtuple: \n",
    "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
    "Pair = namedtuple(\"Pair\", [\"obs\", \"state\"])\n",
    "Pair.__repr__ = lambda x: x.obs + \"/\" + x.state\n",
    "\n",
    "def train_reader(train_lines):\n",
    "    for line in train_data.splitlines():\n",
    "        # a pair is a string \"observation/state\" so we need to split on the \"/\"\n",
    "        yield [Pair(*pair.split(\"/\")) for pair in line.split()]\n",
    "\n",
    "training_set = list(train_reader(train_data))\n",
    "\n",
    "# print the results\n",
    "print(\"test set (observations):\")\n",
    "for seq in test_set:\n",
    "    print(seq)\n",
    "print(\"\\ntraining set (observation/state pairs):\")\n",
    "for seq in training_set:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ0zAoIgiYaF",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a98845e6e8374535",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Vocabularies \n",
    "\n",
    "The `numpy` library is very practical when working with numeric vectors and matrices. Examples in this notebook heavily uses numpy arrays. It is recommended that you understand how simple arithmetic operation work on the arrays. You can also find very useful methods like `max` or `argmax` that will save several lines of code. You might find the following links useful:  \n",
    "[NumPy Reference](https://numpy.org/doc/stable/reference/), [NumPy cheatsheets](https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/) \n",
    "\n",
    "It's going to be very useful if we can map states and observations to integers, so that we can identify them by a number. If we don't do this, then our implementation will be longer and much slower (This is relevant when we work with larger data, e.g. for POS tagging).\n",
    "\n",
    "Make sure you understand what is going on here: every time we look up an observation or state, the `defaultdict` will create a new key (index) if it has not seen that key (state or observation) before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AymV48GViYaG",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-387cb576fbdc997c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1f99faa0-cfba-4711-bbff-a89bdd3c946c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "Our vocabularies:\n",
      "defaultdict(<function <lambda> at 0x000001CD45E3DA60>, {'happy': 0, 'bored': 1, 'hungry': 2})\n",
      "defaultdict(<function <lambda> at 0x000001CD45CF7670>, {'laugh': 0, 'cry': 1, 'sleep': 2})\n"
     ]
    }
   ],
   "source": [
    "# create mappings from state/obs to an ID\n",
    "# Defaultdict creates an entry if it hasn't seen\n",
    "# a value before, using its lambda function\n",
    "state2i = defaultdict(lambda: len(state2i))\n",
    "obs2i = defaultdict(lambda: len(obs2i))\n",
    "\n",
    "# Loops over training set, indexes all states\n",
    "# and observations and assigns each a number \n",
    "for seq in training_set:\n",
    "    for example in seq:\n",
    "        state_id = state2i[example.state] # Converts the state to an ID \n",
    "        obs_id = obs2i[example.obs] # Converts observation to ID\n",
    "\n",
    "# Returns ID of some state\n",
    "print(state2i['happy'])\n",
    "print(state2i['bored'])\n",
    "\n",
    "# Return ID of some observation\n",
    "print(obs2i['laugh'])\n",
    "print(obs2i['cry'])\n",
    "\n",
    "print(\"\\nOur vocabularies:\")\n",
    "print(state2i) # States (happy, bored, hungry)\n",
    "print(obs2i) # Observations (laugh, cry, sleep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5GorW8viYaK",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-430223fd21b6ce82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Finding the Maximum Likelihood Parameters\n",
    "\n",
    "Now we would like to know what those distributions look like from our data. This is called **estimation**. Given our training data, we count how many times each event occurs and normalize the counts to form proper probability distributions. \n",
    "\n",
    "Let's first do counts for the start probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "A_NwY0mFiYaK",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df74082a4f0715ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bf12d78b-f5a9-41f4-fdcf-a6830dd9bca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# we can get the number of states and observations from our dictionaries\n",
    "num_states = len(state2i)\n",
    "num_observations = len(obs2i)\n",
    "\n",
    "# this creates a vector of length `num_states` filled with zeros\n",
    "# e.g. add an entry in the vector for each different state\n",
    "counts_start = np.zeros(num_states)\n",
    "\n",
    "# Now we count 1 every time a sequence starts with a certain state\n",
    "# we look up the index for the state that we want to count using the `state2i` dictionary\n",
    "# e.g. increment count_start[0] by one if state 0 occurs, \n",
    "# and increment count_start[1] by one if state 1 occurs. \n",
    "for seq in training_set:\n",
    "    counts_start[state2i[seq[0].state]] += 1.\n",
    "\n",
    "print(counts_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfaevoHniYaL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba32d826a1471a2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We see each state once at the beginning of a sequence in the training set, so that is why we have a count of 1 for each of them.\n",
    "\n",
    "We now **normalize** those counts, so that we obtain a probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A2mR5lI-iYaM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a818b13defb8e711",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2adad573-a947-4a11-944f-10cfe50182e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start --> [0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "# since p_start is a numpy object, we can call sum on it; easy!\n",
    "total = counts_start.sum()\n",
    "\n",
    "# normalize: divide each count by the total\n",
    "p_start = counts_start / total  \n",
    "print('start', '-->', p_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpjkxRv2iYaM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efc1949dc9262b12",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We now turn to the **transition probabilities** and **stop probabilities**. We count, and then we normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GpYLqNmDiYaN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8f2dba45d15d44c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1f209963-caf4-487c-8bc4-c5be68b3c8f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Transition counts:\n",
      "[[1. 2. 1.]\n",
      " [2. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "Final counts:\n",
      "[2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# we can transition from any state to any other state in principle,\n",
    "# so we create a matrix filled with zeros so that we can count any pair of states\n",
    "# in practice, some transitions might not occur in the training data\n",
    "# e.g. counts_trans[1, 2] gives frequency of trans from 1 -> 2\n",
    "counts_trans = np.zeros([num_states, num_states])\n",
    "\n",
    "print(counts_trans)\n",
    "# for the final/stop probabilities, we only need to store `num_states` values.\n",
    "# so we use a vector. \n",
    "counts_stop = np.zeros(num_states)\n",
    "\n",
    "# now we count transitions, one sequence at a time\n",
    "for seq in training_set: # Sequence is a list of obs/state observations\n",
    "    for i in range(1, len(seq)):\n",
    "        \n",
    "        # convert the states to indexes\n",
    "        prev_state = state2i[seq[i-1].state]\n",
    "        current_state = state2i[seq[i].state]\n",
    "        \n",
    "        # count\n",
    "        counts_trans[current_state, prev_state] += 1.\n",
    "        # note that the order of states/indices in this matrix\n",
    "        # follows conditional probability order p(q_i|q_{i-1})\n",
    "        # not the transition matrix A_{i-1}_{i}\n",
    "\n",
    "\n",
    "# count final states\n",
    "for seq in training_set:\n",
    "    state = state2i[seq[-1].state] # Gets last state of this sequence \n",
    "    counts_stop[state] += 1. # Increment appropriate counter \n",
    "\n",
    "# print the counts\n",
    "print(\"Transition counts:\")\n",
    "print(counts_trans)\n",
    "\n",
    "print(\"Final counts:\")\n",
    "print(counts_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZrxBHQHiYaO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68e67a73fdfab5d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we can normalize again. We will need to collect the total counts per state.\n",
    "Take some time to understand that the totals consist of the transition counts AND the final counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "am85FysNiYaO",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-262f16a6645038a6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7958a6bc-4374-41e2-9f02-ea626dad8d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Useful trick: np.sum(m, 0) sums matrix m along the first dimension:\n",
    "print(counts_trans.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1XsW6QSIiYaP",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2720bcd24a328fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c5cf2ec7-4314-440c-e2dc-0acceaaef5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts per state:\n",
      " [5. 4. 2.] \n",
      "\n",
      "Transition probabilities:\n",
      " [[0.2  0.5  0.5 ]\n",
      " [0.4  0.   0.5 ]\n",
      " [0.   0.25 0.  ]]\n",
      "\n",
      "Final probabilities:\n",
      " [0.4  0.25 0.  ] \n",
      "\n",
      "0.2\n",
      "0.4\n",
      "0.0\n",
      "0.4\n",
      "0.5\n",
      "0.0\n",
      "0.25\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "total_per_state = counts_trans.sum(0) + counts_stop\n",
    "# Print total counts per state, e.g. summing\n",
    "# all 0-index columns gives all transitions from\n",
    "# 0 -> other state\n",
    "print(\"Total counts per state:\\n\", total_per_state, \"\\n\")\n",
    "\n",
    "# now we normalize\n",
    "# here '/' works one column at a time in the matrix\n",
    "# numpy divides all 0-index counts by all 0-index totals per state\n",
    "p_trans = counts_trans / total_per_state\n",
    "\n",
    "\n",
    "\n",
    "print(\"Transition probabilities:\\n\", p_trans)\n",
    "\n",
    "# here '/' divides the values in each corresponding index in the 2 vectors\n",
    "p_stop = counts_stop / total_per_state\n",
    "print(\"\\nFinal probabilities:\\n\", p_stop, \"\\n\")\n",
    "\n",
    "print(p_trans[0, 0])\n",
    "print(p_trans[1, 0])\n",
    "print(p_trans[2, 0])\n",
    "print(p_stop[0])\n",
    "\n",
    "print(p_trans[0, 1])\n",
    "print(p_trans[1, 1])\n",
    "print(p_trans[2, 1])\n",
    "print(p_stop[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1cikCNxiYaQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62194d4b1f2fc0bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**So far so good!** Now let's take care of **emission probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "R2UmIZEkiYaQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1273aaee-10b5-411d-e299-2625564f9831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission counts:\n",
      " [[2. 0. 0.]\n",
      " [1. 3. 2.]\n",
      " [2. 1. 0.]]\n",
      "p_emiss:\n",
      " [[0.4  0.   0.  ]\n",
      " [0.2  0.75 1.  ]\n",
      " [0.4  0.25 0.  ]]\n",
      "0.0\n",
      "0.75\n",
      "0.25\n",
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# now we create a matrix to keep track of emission counts\n",
    "# in principle any states can emit any observation\n",
    "# so we need a matrix again: \n",
    "# 1st dimension is for observations, 2nd is for states\n",
    "# the dim. order follows conditional probabilities p(o|s)\n",
    "counts_emiss = np.zeros([num_observations, num_states])\n",
    "\n",
    "# count\n",
    "for seq in training_set: # Sequence is a list of obs/state observations\n",
    "    for obs, state in seq:\n",
    "        obs = obs2i[obs]\n",
    "        state = state2i[state]\n",
    "        counts_emiss[obs][state] += 1.\n",
    "\n",
    "# normalize\n",
    "p_emiss = counts_emiss / counts_emiss.sum(0)\n",
    "\n",
    "# This matrix that shows the probability of emission  \n",
    "# given the state e.g. p(obs) = [obs][state]\n",
    "# e.g. chance of obs 1 is [1][current state] \n",
    "print(\"emission counts:\\n\", counts_emiss)\n",
    "print(\"p_emiss:\\n\", p_emiss)\n",
    "\n",
    "print(p_emiss[0][1])\n",
    "print(p_emiss[1][1])\n",
    "print(p_emiss[2][1])\n",
    "\n",
    "print(p_emiss[0][2])\n",
    "print(p_emiss[1][2])\n",
    "print(p_emiss[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX4u4L3YiYaQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc1d7ccaf0a06283",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is a good moment for a sanity check. First, take a look at the training set to see if these probabilities are correct, i.e. check if  for each state $s_k$: $$\\sum_l P(s_l \\,|\\, s_k) = 1.0$$ Note that this includes transitions to \"stop\" state, so you have to take those into account.\n",
    "\n",
    "## Exercise 1: Sanity check (2.5 points)\n",
    "Write a function `sanity_check(...)` that checks if all distributions are correct. If (and only if) it discovers an incorrect distribution, it should throw an **AssertionError**. \n",
    "\n",
    "If you want, you can include some print statements to see what is going on. \n",
    "\n",
    "**[Python hint]**: use python [assert](https://www.tutorialspoint.com/python/assertions_in_python.htm) statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "YdRz5AEXiYaR"
   },
   "outputs": [],
   "source": [
    "def almost_one(p, eps=1e-3):\n",
    "    return (1.-eps) < p < (1. + eps)\n",
    "\n",
    "def sanity_check(p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "\n",
    "    # Get all states possible\n",
    "    states = len(counts_trans)\n",
    "    sum = 0\n",
    "    # Loop over all possible pairs of states \n",
    "    for i in range(states):\n",
    "      for ii in range(states):\n",
    "        # Get prob of state transition\n",
    "        # and increment sum\n",
    "        prob = p_trans[ii, i]\n",
    "        print(\"Trans prob from {} to {} with prob {}\".format(i, ii, prob)) \n",
    "        sum += prob\n",
    "      # Add transition from state to end state\n",
    "      # then print results and reset sum \n",
    "      sum += p_stop[i]\n",
    "      print(\"Trans prob from {} to end with prob {}, total sum: {} \".format(i, p_stop[i], sum))\n",
    "      if(sum != almost_one(sum)):\n",
    "        raise AssertionError(\"Probability did not sum to one, state: {}\".format(i))\n",
    "      sum = 0\n",
    "\n",
    "\n",
    "    # print(p_trans[0, 0])\n",
    "    # print(p_trans[1, 0])\n",
    "    # print(p_trans[2, 0])\n",
    "    # print(p_stop[0])\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iSkbvB8niYaS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c51b4f1a-a887-4f5a-9c68-9589aaf9c0ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans prob from 0 to 0 with prob 0.2\n",
      "Trans prob from 0 to 1 with prob 0.4\n",
      "Trans prob from 0 to 2 with prob 0.0\n",
      "Trans prob from 0 to end with prob 0.4, total sum: 1.0 \n",
      "Trans prob from 1 to 0 with prob 0.5\n",
      "Trans prob from 1 to 1 with prob 0.0\n",
      "Trans prob from 1 to 2 with prob 0.25\n",
      "Trans prob from 1 to end with prob 0.25, total sum: 1.0 \n",
      "Trans prob from 2 to 0 with prob 0.5\n",
      "Trans prob from 2 to 1 with prob 0.5\n",
      "Trans prob from 2 to 2 with prob 0.0\n",
      "Trans prob from 2 to end with prob 0.0, total sum: 1.0 \n",
      "All good!\n"
     ]
    }
   ],
   "source": [
    "# you can try your function out like this\n",
    "# (do not use this cell for your solution)\n",
    "try:\n",
    "    sanity_check(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "    print(\"All good!\")\n",
    "except AssertionError as e:\n",
    "    print(\"There was a problem: %s\" % str(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsBjY3dyiYaT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65c6613cf4767c2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Decoding a sequence\n",
    "\n",
    "Ok, so we have estimated a model. Great. Now what? Well, now we can **decode**! \n",
    "\n",
    "Given an observation sequence $o_1, o_2, \\dots, o_N$, we want to find the sequence of hidden states $s^* = s^*_1, s^*_2, \\dots, s^*_N$ that **best** explains those observations.\n",
    "\n",
    "But what does \"best\" mean?\n",
    "\n",
    "1. If we are interested in the best **global** assignment of states to the sequence as a whole, we can use the **Viterbi** algorithm. \n",
    "2. If we care more about minimizing the **local** error of getting each $s_i$ right, we can use **posterior decoding** (also called *max marginal decoding*). *(This is a bonus exercise at the end!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQpYp-cCiYaU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91a84fbaef83d741",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Decoding: The Viterbi Algorithm\n",
    "\n",
    "Viterbi gives us the best global hidden state sequence, i.e.:\n",
    "\n",
    "$$ \\begin{array}{lll}\n",
    "s^* &=& \\arg\\max_{s = s_1, s_2, \\dots, s_N} P(s_1, s_2, \\dots, s_N \\,|\\, o_1, o_2, \\dots, o_N ) \\\\\n",
    "    &=& \\arg\\max_{s = s_1, s_2, \\dots, s_N} P(s_1, s_2, \\dots, s_N, o_1, o_2, \\dots, o_N ) \n",
    "\\end{array}$$\n",
    "\n",
    "To explain Viterbi we will make use of a **trellis**, a kind of graph that shows us the possible states for each time step. \n",
    "\n",
    "For our earlier example, the trellis looks like this:\n",
    "\n",
    "![hmm-baby-train-1-trellis.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAs0AAAFOCAYAAAB9gyBqAABurElEQVR42u29XWxcaXoeWHD6Qgh0IQS60IUuBGxf6KKRyN6GoQsBISDAarJGJHu0HY5XHlMTUkOKag9ta2fksewhHaqq1K0JuAs5KwdEwA04mZJaztIwLwRHMYiE69XYzIQ7qxj0Lo1lbDZUp8g1uAlt02N2d+33HL4f9dVXp4r1c86p8/M8wAeKVP2c8z3fz3ve732fN5MhCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIIgCIJoCz3TmyfeK5TOfalQ6uktOEPZfGn4qOVKWfz9SmH77Z7pylvsLYIgCIIgUosbN26cGB8fP6daz9jY2NDXv/71YaNl8feRkZG32VPxBgzjvpwz2Vcoz/fmneW+vLOu2q5qleZbeUv9fNGbLz3se+AMXP3u69PsWc5/gvwTBEEkChMTE2fUojii2vzNmzeX1YK4rtquapUW2qZqz9X7Z7CgqgX3FHs2uujLbV9QRu6MamutGcctNRjfhfcebF9ij3P+E+SfIAgiloCHQC2Sd9UCt9JgIdzHYigLaRGLqtGW5O+bDd6/pto9eiOigfdyzuVs3nkkXuFKuK28BS9070efniUTnP8E+ScIgog01EL3rngCNjwWx0XVxnDkpn6eb8VTMD09/ZYswgPqvQVZTPet71hXf5+kB6I7xnKbHuV91TYRsqFaEeEbRy3vLGVzpZU2DPAD97Puv36XzHD+E+SfIAgicl4FWRTNRWxHtTksdIhd8/s7sZCqz76M75Dv0t+LY79Zeh+Ch4RgvGjGkHVfl3Mmkdx3Nf/6/OD0btObGxIBkRCojOlryiCe7Ss4L8XgPu57l/A+MsX5T5B/giCIrgKxarJoHZgLlmqXsKiFdR34LsS5qfbCWDwP1KL6kJ4H/3FowDoLjQzWbN7Zg8e3t1C+3jO9fdLva8Bn4rNxHfiuxp7n0sNWjHSC858g/wRBEL4AC5FalKaMIzIsmo+wiHb72m7evHkBMXHGQg4vxFiYi3hS4Rqq8PTW9/IewIhFuAbk5MK7rs0TynAeUd//qlHMM66LLHL+E+SfIAgiFCALWi1CJf1ErxapZ1E8CsPiKbFv2vPwYnR0lElibcL1Lh+qVdQxSkvPEa7R7evM5spXRNbO8zrhdabuM+c/Qf4JgiACgxyBzRmL5TISP6J+3eo6rxmL/A5i7Mhma0Accl/O2aljiK7i/6N2zYh/zuadjXrX3Pfx9hkyy/lPkH+CIAhfoRab08ZT+5769/W4XT80Pg2vQ4GsNml85stjkshnxyxvoIpflK8dYRuiFe15/UhIJMOc/wT5JwiC8AXj4+PvGFqZWzj2iuu9QJJIx7oh7o1xbo0MzspbfbnSY09PbaE8H6cQBxQ/gbSdx72Ueu87F8k25z9B/gmCIDp9QkdG8p4sMi+jkOjhw8J5xbinJXVPJ8l0NVCiuk5c8EFvrnwnjvcE9QxI0HmpfLCiIOc/Qf4JgiA6fSqv6KfyILQ2u4XR0dGLOs4NCyc9Dm8AD/JhUZFa4xJJdnG/t76cM+flcWaohuf8P+D8J//knyAIogGk8pLWurybxHtEVSpj4Zwn64eoY1SuJ8mo7CuUpzzucZPJgZz/BPknCIJoxcNwQR9fwduQ5HsVj8Mek0OODObJGg9zrrSSxMIgdQzntbTL0XH+c/0n/wRBEE1AKjxtySIyl5JN4opxDJlaOSKEXnioTKwHUc0vOg8JtYmOKN7C+c/5T/7JP0EQRF2IDueKLJgraYrzUvd7W5eBHR8fP5c27hF6oQzGXcuA3EVBkyTftxvjnHcWPbzrWc5/zn/yT/4JgiA8IaVHsXBsJiFLuo2FczGNG4aoStiV/g7SUnIanvQaOToUcklZfDPnfzrnP/kn/wRBtAhDKWMPupxp7IMbN26cMo4m76XlvqG5XBPbmyvdThP3ffdfv1sTmlJwFjj/Of/JP/knCIIwn7BRLWlXEj+upbkvxsbGLsuiuT86Ono26febzTnv1BiLOWcujdxnC6W79sNDGvSbOf/TO//JP/knCKL1RXNWFopF9obbH0Xpj2LS79Wj2Edq1SMkvnk1bf3B+Z/e+U/+yT9BEC1gZGTkbckcRmNxh4wrQ3RWyxCNj4/3JPU+v1Qo9dQmwMW7eEmnkHLbVlGX0jDnP+c/+Sf/BEHwqbpIcXfPfrknx5XLSb3HvoLz0pJaWybzbr8sWJUQN5Lqbeb8T+/8J//knyCIFiAi9m78VhqzpRtB9cdJHeenWuLiWvsK5Wu1yX/bF8i86puPt8+o/tiv6psHzgDnP+c/+Sf/BEGk12helqfpGfaGp7dhKomxfvCawntqeZkZv1f9UGEriqxx/nP+k3/yTxBECmFkCe9Aaoc94rloIqt8X2LbEiPD1FtwhmxN5qQXMWkV6I8kK2lw/qd3/pN/8k8QROsLwhz1KI/HzZs3HyfNGwOvMstGH4+aSoG50mPOf85/8k/+CYJIEaRc6g4WA2RPs0cabi6XZHPZSsL99ExvnsjmnT3GMh8PlNK2qwQmISGQ8z+985/8k3+CINpfCDbYG03112ZS5IdQGrs67KDMzaDuA0blLbecdsISAjn/0zv/yT/5JwiiRYyNjT3kkVPzQD9Jf8X+eB7V/iw5tUdkuD7QP9WltcvzSZn/qhXIcLrmP9d/8k8QROtPzhuU0mkeo6OjF6W/1mNvNOfLW1XJbTnnMhmuD48CMJuc/5z/5J/8EwSRAiALWBaAEnujOUgMoKvZiWpRcb2P3vvORcsA3E1ryexm4RUDHmelEc7/9M5/8k/+CYJo3ctwTxbNOfZGS/22KP0W25LKyuCbqQ41cBbIbDP9VnpuxYGPcf5z/pN/8k8QRPIn/wuZ/Fn2RvO4efPmZNzj2lAmu0pqruAMkdnjkS2U7iYlrlnP/7GxsQEym675z/Wf/BME0fqiuUGpodahiwGony9ja/xZVQBZ0KTJfsuVr1ge+pec/5z/5J/8EwSRfKN5D5N/YmLiJHujeaj+OqMraMXYaK6Kze2Z3uYYaAK9H3161o4F5/zn/Cf/5J8giGRP/JMy8fcC/ipkZYdZMCOU79MbDsqrRmQDHFNtCgv6ca+FgWxJzQU2BlBuOsyCKWF8n/3AcfW7r0/HiX/Of1/6ezeu85/8J2/9JwgiYOBILiRRe8R8ToV4a6F8n+q3VcmgvhiRTXNK+NxHYg8y4+u9FqEYltEc2BhAzK9qofEfxvepPlurkupThnqc+Of8T/f8J//J458giICBikaSzLDMRbN1oN+iVBnK2DTN9lxd5xX7tbbeMJICaTQ3DzuJEv0ZJ/45/9M9/8l/8vgnCCL4SX9dFtagpcawiBVVw+K8YS1oiKWbk7+vqGYuQMvyWl1AAseOC/L7ovwOQFt4Vj7jufG+oDepBdl0rkdx01TX9YXx+6uxsbGRGzdunHCNvkL5elhyczBilZFZhKHpJh8aBq0bJpJz5vD3bK60YhqfrmGqXqsLiPR9vH0G14nfVVvE74efUXlLvXb2MLGx9Fy/L1jD3L0OQ3mkfD1O/HP+p3v+k//k8U8QRPCT/puSAfwwhEVzVbVzqp2XxU0vNEuqQbYHizmq0SGpSmdyV1RDaVd9zIhM5bvy2nuyOGbk3/i/s/LazTAWTdVv87LpLMqG1dWmrmfZw9NUsTbSbbx2cPpH09We5lJgY8D1/Oad1fcKpXNX86/Pw7jVRqb6+1JfrvQYRUNQjRBJdVrFQ65tJptz3hFD9SXk3twCIwXnnvaO49/4PyTo4bWuUR240ezeU+X97/yg8pVvFSvDkx8txol/SWS6w/mfzvlP/n3ln1rNBJF0YNFUC+jvy6QPeoGxj8tui6fgtCyMZub2nCyUetHUOC8L6jlpb8v/n1JtDbZTg+8L6qFj7rhNKkLt86rfx24d/Ow3FyrZ/GutNRxYf9nhEspIvg1PMZLnbNUOeJ1hKGujWf8dxjYMahjeaDome3B69xTii7O5Urbe9wVyT4fX6RrMseT/MO51hfM/pfOf/PvGPzz4tCgIIuEGs5rs62qy/ztZNAshL5rD4iXA4rdlvXZKXm8vmj2q7cv7zHZGPAs9XVg0i7JoPouopwkb5V9af9vBYj/0q78zd2QwH7bAxoBtxGbzpWF4iWH89uXLW9Zrp3SxENNolhjsfbzPbG7IRt7ZNMM6QoppLlZ5mr/x4Fmc+IfOrGrf5vxP5/wn/77yz6JQBJF0g1kWSz3pg65oZi9iOE5bEC/BgXgcNJ5lDo/g6nkaTnh8Po7mroW9aEY4Eejzehvl9PT0W9pwDauqXY3RfBhOsSBe4gNTri2bLz9DCEY9TzNCM2o/33mpPv9ayEZzlBMBj+VfXj/M+Z/O+U/+k8c/QRDBGsxo90PMnoZHYUAaBOF1Rjfi2RDXBtme2/K6Mx6LJvBcFlW8FjFxLwzPxbp8Jv6+F5KnYVMWzXMR2zTrbpQaoatn5MtbfQ+cAbflnB1U1XP/L1d6jLjm3vvOxcOwDfU6SfAzjebD30vPYVTjtZLI+MJ4AFjHZ+LvroZy8JJzm1WSc4XSuTjxD4SsnsD5T/7JP0EQsTWY4WH4efn3esBfPyKegFlZIM3jLCzok5k3CSFvW4utiZPipcDi+ch67Zj8vSDfNxDCoom4wIqZkd7lTfP2cRulhnhuzfCMwMaAMmJH4AmGwoVrIBecI/6hfKGM6EmdEGiW8ra934h9dr3UrvHsPKp6bb48hr8jzMT9PmWcB2w071dXU9w8ESf+5fXnOf/TOf/Jf/L4JwgiQIMZ7Wtf+9pPyb932UutQS2Up+LcdxIakYhS0Oy7dI5h9h3vgX1HEITvBvPY2NifeGVUSxlVPi2352UIy0sTGKLoLY0DwvTShzCOOf9TOv/JP/knCMIymNWk/pM6EkR7MvkZl9UGQowHDNJojlxcbhwQZjx4CJs/539K5z/576jffkb20U32BkEkx2D+vxrodkJg/igDuF6pVaLuojkWUuZ5YLAVIIKOA04KJAY7FOWRoMH5n975T/7bx+jo6H3h//ORkZG32SMEEW+D+aRXDLNVGWpZFs0Z+dsse66lTXMx7sL2kpj3xvjLOXNktgmjOe+8sPptMsZGE+d/Suc/+e+cf2nP2SMEEXOoxfCCObHxRGwZzgvyxHyRx0ytAfF/OhYQHv243ocdZqBaiew2BhQ8oC2dlLAWzv/0zn/y3zn/al/9L/TUE0SCoBbFIS9Ps5rsD42n5i2Z+BfYY8dD9d2A9ONKvA1AV+5tp8oAfLB9iQzXh+hDmw8aa3G/J87/dM5/8t85/6rd0QmBzcj8EQQR/QVxURbE71lHSnf0a9T/PZa/3WOPNdWnc/LgcTfu94LKfJYRWCDDDYxmKZ9ttJm43xPnf3rnv8X/FNltymie1/zDUEZ+kPTfbfYOQcQYxtHbnj5G1GEb6ud1YxG4LK9bY681hiySqLZVSUICiFuhzzACs3lngyx7A5J8brXBqnjm7dh75zj/0zv/yX/n/Kt/Z7Vms2qn2UsEEV8PwnIzHgSJ0drDa5WhfZY9Vx+qjy6Z6iPxNwTdGN0qvWaz0h7xBu/lnMvVXubyVhLui/M/vfOf/PvDP5IB5UT3MXuJIGIIrSOKp19UL2piMViQST/J3mvYT7PSTzNJuSe3hHXCQg4C6ScrlAWlvBM0rjn/Uzr/yX/n/EuxkwM0tfe+w54iiJhhbGzsZStximoRuKazqFkdyhuifb0r8WzvJuW+lPE3YhmDe30fb58h44bBnNu+UKOakXMuJ+X+OP/TO//Jvz/8q78/kj58wd4iiHgZzDq7t9TKAoiYNjtJkPBcFBeTdF+I1YXcXHWsbonHjKbRnC89T5pqBuc/5z/5949/nOgaRjULRRFE3Ba/Vo/aWg3pSBOQ9KGP35JYAcr2NsOrytjmQ3joWSfKy8z5z/lP/v3hX1eKRMwzPfYEEQOoJ1yty7zVzqTF0ZKt40y4x5fPpF8TWTXP1WzOO+vVYRrlZ2TejfletYzmxB6/cv6nc/6Tf3/4F3WNV0mSJCSIxMLSjBxuc3G4IO/fZyb10YPIu7pP4l4BrKFxaMnPofXedy6mmfvegjNk90kSZOY4/zn/yX8w/BsSfrtJHy8EEXcvwbA+GuqkOpEh3j7PXm1eui8JyOZKK1VGc95ZTivv8L5Dt7rKYC44CykwEjj/Uzr/yb8//Ku+W0rDyQRBxBaitdmRl1ljfHz8HJ6qRT6nJ+UL5nUjqTLxcX4oo13jWS2UU1kpTN33vNUX+2mI8+b8T+/8J//+8C8x0PtJVFohiKRM7kk/qzpBh1IvFmk9ppOjyj1Z+EZSYyzmnUUPw/lamrjvzZXv1ISq5J3ZFI19zv+Uzn/y7w//6n0F6cMVWigEESGIl7nkp9SNxEc/14b4xMTEyTT1qWhybqXxiK33o0/P2hJ0rnZzgmN5TWRz5Su2JjOSJFE9MS1jgPM/vfOf/PvDv0jQ6X15iJYKQUQEyNKVyb3q88JxUn3mepK1Ses9hOjiMIhn6yQ+PLaG833nol1eG2Wjk1705Gr+9Xn3AaEq8c/ZSaP8Hud/euc/+feHf3ioWTSGIKI1wY8E1YOIP5PYrN00JsJgoVPtdFrHVm+hfL02TMN5iQS5JN7v4PTuqZrEv7xzAJ3mtI4Bzv/0zn/y3zn/4rFfTVP/EUSkgYmon4gDXEQui7B74o+Z1P19Wxa4PfUQ8k7ax5cyGgtvQjReH+k3J81wRugFlENwj1/K/bnpXR9L+xjg/Cf/KeL/rt/8q8+6RBk/goiGwXw6SC+z9V260tFeUhdOtaD9irq/L9QDyBe4X46wo6InS1dn/rQy9K2nb2KcIU2XkFANhF4gZjl7f7Py85MfV0Zu3an0/8Z/Yinx6vn/Tc5/8p9k/iWZ/kBa1uf+K0r/LXA0EUT3FrLZMOPNjO9L1FGTHKHNGffmbg5qEb2W5vGFmEa3tO6tX/j45viHn//cL/1mpSbGOebJgSiHre5lF/fz/nf+8Ij/m2O3Prs5fvsfkX+3tPJ91f6rEd/K+U/+E8s/jGe/v8OQ8auoh7NUF4wiiG4taGf0JIQ0ToiG+pg+qoOxHvesaunHFb1RShybuXnOpiURCPGLokv6SLUfqX9/ZvbFV75VLNsxzm7SXEzl6Ppypdu2SsbXPvxOhfx786/aHc5/8p9k/v1Sn/KCIeO3miEIIlzIwtaVrGaJcdPJIWtxjdNCzJqxSW7h4cPIdv4zY3NYSVo5VBgC8CJJDN+iajvWBlnT/vHY5AhCNWqSA2NWAMUNOck5c1738bPf+v6/J//eDQYV5z/5TzL/ARvoJw0Zu+EMQRDheQR07BX+3cVr0Md1pbhVjsLRqxauNzdF0bx2NxB1j6PGIldKUnUsbJpaVqlB+8L8HfcvMc4FL4MTJbchVRfl+xYN5jWP6z9QhvQk+W/MP+c/+U8y/yF871GFwbRpXxNE16AlcfCzm9chT85aAP8A1xN1r4O6zvPiWanoPrSPX41KTkVJtnxh3OPdpIwj9IW6n4NGG6f6/8/1v80HtGy+NFyr4+wcqWtETdcYsdcw6j295HlnF8Y0+W+ef85/8p9U/kPYv7UG9AytGYIIZ9J31ctseywkVmtfy+qoReEh9KOj1G/wJMjDht4kduslfGDh132M9xn3WElKLJ8xnr553LGsbvY9SwGUUh1D9ADqE91W2IDxrozlYp1rdCv9oaAJ+W+df85/8p9E/oMGEgF1X0VhDyeIpBvN+il5NkrXhexgQxS+IjFvd7pdBQmLt7WoH4gKSEPRevWeZ7Y3ADJERizfRhJ0XG/duvUP1b38uIlN86+93i8ltxfrGaVSXW8mbM+zeJZnPcphvzHqC+V5FDQh/1//23b55/wn/0nkP4R9fEHG1zNaNQQREJCooJ9Qo5qYItf4wlg8N/FEH/axncTc3bMSXIrNPtkbgvQlc+EX6aA1I9s+tgkdkgR04BW/6NH+rNFnvfdg+5Kr3Vzfo1uRansFvNbve0GsNeTj1Hc8cqXwGlwHkhmzOecd8t8S//+Z85/8p4X/oCGqHXth1FggiNRCLXJLUfQy1/E89Bibi274/V5QWcrY6MSrsGFlfS+38536+nXWuOG9OGF6VdT/P46TLJWUXl80rv+fqp/bx2TOv2zms/seOAMIeTjGaK24YR1QsFCvf69QOtczvdmSRwoV/NzQi4Iz1FdwFrTWcsNWcF62UhKb/LfOP+c/+U8K/0FDjHq3X9Iia0gQYRuh7rFX1OLFjvE8XJOjqD1rAYXUz6z6/yvw3rR6jIf4OvEmZEWYvmR9PjwMc5BH6uAhZaSRrqb8vz72W42D9JK65ncNmaVdXfnq1q1b/63691802DiLzRu0lbeyeWekQbxzvQbDd91N2CuU512VjkJ5qjdfegjDGH+Ht1pCPpr+XNfD3YaWNPmvMjy+z/lP/tPEfwgPLycMLliBkiB8Nj6X41yJSbwzl+GVMWSc7IZFfB33Kp4cZLFPIbEEC6/0wYbHAqzbhrz2kh9P7qb8VL0qTnIcqRe+HWwCUeVAChMcbfLYrKz/R5Lp/2t7aiTL/GGr3wfvsRjPS/WUNoJqMKyRAAhPNIx48h8+/5z/5D/O/If0EDOkx06cnGEEEWnE1cvcCNiEcJQmC+GmsZg32/ZkAUX83L2gknJM+akGG8IpU3opag82Ig1VbOY4+datW39fuEA1sN8zq4F1cg0Ip3BDNxBKkXN2gjGWUdbbmUNsc6vhHuQ/WP45/8l/HPkP0yHW6YMJQRBvFu61IBauqEE2n/PykDAsiSpTuG/EFeLvOJILU+7Jlp86hqcpI7HmeRQecKTq1SujTOzQMfdwz6w0KZ60RTuuszMDuvIWYouhbiHayZuteqLhSUbYhbx/JqiiKuTff/45/8l/XPgPyWi+ECUZWYKINdTCMeCVxU2Euqg9a1aMXkrM6oztTcQQdvFha9g4ysTGeb7R6+F90rGB3YgFhPwbNJNhUKN4SrZQuuvGNOfKd3oL5ev4O5L/4LUm/8njn/Of/KfYMTYn/b7E3iCINiGLmM7gnmSPdG1Bu9TKg4t4p1YNsf+RMK8X1yixg0dVr5rxzhjxda/IOvknyD8RDkSCblf2+ivsEYJo31PgJjhQkqbrXHjKTzV64LE3rjBOCiSjfK2dDVuXd2UmN/kn4+SfCH2M3ZH+X+d+TxAtQrzMOot5mD3SXRwnP1UP2GSNI9K1IGPWJJTnqGJZK9qkIkWlk00ZBkT+CfJPdG/Pv80eIYjWnjrpZY4QmpGfqgdJxtmwtVF9XmxnjczyxVaTZXRZV2Zwk3+yTf6Jru372aQpZRFEWAt0SY4Dr7FHIrOgFVot9KEhsk9mFa4ZPx6GJH5yRctdtRP7LvF0kHw6sLVbCfJPkH8i1HH2QksDsjcIoglg4dPHeeyN6EA2l6bkpxosiHcMWaoX7X6OjJMrRqb+VqseMOOaqmSmCPJPkH+ia0bzeT3O4qw/TRChwPQyI0aNPRK5Ba3YrPxUPYgOaandzU6OY6s0YVU73c61UGaK/JN/8k/+IzfOHukHK/YGQTSeLFOyKC+zN6IHbHBG2dO2E2bEa7ViVBG73cL7XvhVfYwyU9Hgv9ljdfJP/sl/8iGFXnRSZ5Y9QhDHTBR4I9gjkX2wWRXPTEf6q/DyIPHGSOApNkrgEb3YLa0Z64dniDJT5J/8k3/yH8lxdluLAVDRhCC8Jwm9zDGAyEj5FnOOZM/jKndJSVl9HLvSSSyk8ZmUmSL/5J/8k/8IQkJnXskD2l32CEEYMDKYK90svUocDzPuHN4fnx6YzusFEhuoVk2R04dFwxtV8EuCkDJT0eUfx+bkn/yT/3RDyrK7DzZ+PCgRRGJg6GwygzkGQCKQnAo88/HB6aTeyGQz+776+Z+D0HelzBT5J//kn/zHwnBekjEwx94giOoFrNJKFSei65y5skDQSvX5AQqxbJ8b3qX/w++NjTJT5J/8k38/jX4iGEhpdHes8RSaIA6fJOdlcVxgb8QHWn4KR6Y+bsYnjc/VHqeSn4mhlJmKH/9+8kT+yb/f4SVE4GNNF9ZZYW8QfIqUp0j8mz0SH/glP6Uh5XaP4hpVGzXlpfxKBqHMFPkn/+nl3+9ERiJ4SGy7ftAdYo8QqYX2MuMneyOWHgBf5KfUZwx7ZdDDK6TjJ/VxeiNZqibHHGWmyD/5Tyn/fl0zEbqtMCJjYJNqJ0Qqgfhl8TLv+x0XR4TGYUdeGyx+6jMeG0ex816bIpKADLH7jXbLqxoyUztceOPLf7u5D+Q/3fz77R0nwoOE1awJf/fYI0QavRRaSmiWvRFPdCI/JaE5ehHcP87zg2Qg4/XwSg23MeYW/I7DJP/kn/zHhv8i+Y+1zXBJjxc62oi0eSgu6MFP/cXYc9my/JTaIAfa8RxhkzYSR/Gdj5vVbQ0y45/8k3/yT/6JUAznIoUDiDQutMuy6M2wN+KNVjYjOWKb7TRGUeLb9uUzVpvZBIPQliWiwX8zkmTkn/yT//hDThxc7hFuwx4h0jDoe4zypafYI4l6+q977IkNFZJBOhtebV6THT544bRiU8coqt+v1HttEFXMCPJPkH8ifOgHICT1sjeINAz4ZVm8ptgbycBxCTbY0PB/8potvzwEIkX0XG/E9cYUZabIP/kn/+Q/GRA97y3hdJg9QiTZI5GVgV6ilzlx3NZIOclx7JRsaBXZ4E4H8N1V32GPLcpMpYt/+zvIP/kn/8mC8SBU6lSGkCCivLCuSVzZJHsjsYvYmngDzpjFCYI+WUDVMMObtalLrlJmivyTf/JP/pMHrbnO3CgiqQN8wPAyc/FKGMy4QbWITRjHZ6WwyhVL3OSqKWNFmSnyT/7JP/lPHowHov1mkkEJIjaQY7oNHpElG3JMik3zC1nMVsKWFJQqYmbBhM8pM5Vu/lUj/+Sf/CdzzC1QFYVI4sAe1pqczepqEvGClZSD9pvd5BrHxar9WCu1oJgCWSL/BPknkgMJA3JLsEOZiz1CtLZwzfecGC8OnvuwONgzURwYuvW0f/ioFfuz7t8/6X97erkntMXM9DIz07U+eqY3T7xXKJ37UqHU01twhrL50vBRy5Wy+PuVwvbbPdOVyD10SHliLf/04ygch8px8Y6xiaOYQpYjjfwT5J97QHKAsto6nl4/qIkxPRumlvNyJnNiLZM5t6qo/GEmM/QfMplho2Xl72+r15G/sAHDeOJJ/+TEk4H5W08GltXPddV2Vas03Z4ObKmfL9TPh7efXB34+vevng5oQN+WAf2KXuZMBotiX86Z7CuU53vzznJf3llXbVe1SvOtvKV+vujNlx72PXAGrn739ekuLlhjZqEBtYFeNXS4uxa7biQm/cgo2e4mjdQbh90en+gvxObBY6L6cUhOaHTL4u9R85jFgX8c3RrGU4H8k/9u8p+0PaDbkAekTQnH+WUpoLMflOgADGNlBE+qNq/asmrrqu2qVmmhbSkD+oX6+VAZ0wPqZ2r5CwwTnwxeuFUcnFGG7lpLxnFrDcZ3YeKT9y/5OJhLMpgH0spdX277glrgZlRba21hbKlh4S2892A7FPF+0cosepW01VnN2FC7uJlXyUypf98xZKle2LGW4plYDUMKEd+F60JJYNEtXzfKCjfbsEk8F6H/bNgSjuSf/JP/dO8BkXIkjo+PCbdfWPPEl3Lbf5TJXFCG7Yxqay0ax600GN8F1ViAp13cLg5enngy+Ei8wpVQG77z6cDDX/jel9tOoMBTXlpF5d/LOZezeeeReAQq4bbyFjwQvR99ejagBeodnBwIt3vwipn/L14y93ShG31fT2ZKqlGW7CIL2OyNjf5RENcED6H6jrtGVTSvBu/IphhSRRhVRluSv282eD8kHe8F7Y0k/+Sf/Kd7D4iMM1HCMAzPMozmz4w50fYY/I+ZzOUfZjKP4BUO0FCu64mGF/oHmQwTWJs3ltvyKO+rtomQDdWKCN/Q7daT/qWJYv9KGwb4AT7rw2L/uy16mU/pmLJGJU6TaCy36U3YV20Tx3WqFXF0d9TyzlI2V1ppY/E9cD/r/ut3/bo/OSre0wuSauft10gcuz5huBw2B41kpmSRXTH0YxE+NGcssgd+xcEh1lM8gRsexhFCRsZkIz/fiocL/StGGGQcC2JM7VvfsY6HVr89Z0ng36hKeiAP9uQ/gfx76feGwX/S94CoAEVsDGnDiscccFurxU9gLLfpUd5XbRMhG8rYLkr4hm5Lqq20YYAf4LPUNSWOP3+emj4ZvODGGTdhyB6+rn8SyX0ffv/q+V9cHGx+0V3ueQsJgR8+HbymjOlZ9VkvxeBu+L2u4a3e1+SA1vJDy2ngTo7fXjSziLmvyzmTSOy4mn99fnB6t2nukASCZBC1kF5Ti+FsX8F5KYvtcd+7hPe1e3/w2FhSTvONFiMjOWOxC56Hg0Yyc+JZetjIY9dJfCMMGjOOWnu9sDnD0Aki1lPu6bIYAHYC1Gyn3kfyT/7jxn89ubug+E/6HhBRw/m84vJPjwllaircASEYEmfcjCGL100iuU/9PL+WyTTNHxIBkRCo3ntNtVn1/pdicB/3vUt4Hy1l11juf3ui2L9wjNG6d+g97r8+8ckHJ/2/hg9Oup99eB17DQ32pwMPGxnp4mXeTYMMzOHi5Sw0WqyyeWcPT/u9hfL1nult37nDZ+KzcR34rsZeh9LDVhZowwhYM4sGNLl54cn/IEzhefHsNaXbqV7zbY8F9m/l5502N+w5I3ZyV44OL4WZZCaevqxRkc31oMFQaMfzSP7Jf9z4h7c5LP7TsAdE2n46zBF4Wc9oPi4ZUAzYhWMM1j14fFW7/seZjO/84TPx2XIde40MdoRttGKkJ8xY/uCkeHr36xqoyohFuAbk5MK6LnzXRHFwRH3/q0Yxz7gur/cbT/HPk8qdu0jhKb/+E/4BFjAc1UFKKLzr2jyhFs0R9f2vGsW74bqa+Tw5AtZJShtqAbrQghdgQbxSD0MZt0bi6XHeBdnUSw28E3vNbvbykDhlHA/CaHoUdmGHOobBBXgFDUMOXsixZo048k/+48j/cSEWfvCflj0gJobzSY/TnYbJgDBUxdNbz8t7ACMW4RqQkwvrXvBd6ntH1Pe/ahTzjOtKn3f5UK2inlH6HOEa3b7OD58OXBFZu3rX+dDUfTY8DJVWFtjYeZcPM5XrLEil5ziq6/Z1ZnPlKyJp5Hmd8DjU0/wUb9WssfAsthobJvqtoclPGTJTa8d54hp5JnQGNpKvmrjHEXPzhYczioUUMBeNWE5XQaBRlTTyT/7jxr9RRGs1aP7TsAfEDTJmH3nw+crLuyxqFZ5GqRpAzxGu0e17Utd5RWTt6hnPD1Oh+yyJfjt1DNFVxClH7ZoR/6yubaPeNU980ndGFq7ZbsSyhQU3ySPn7NRZhFYRoxa1a0bsWzbvbNS75r6Pt6s8YthMzWS5TrQuw5Sf0jJTxxXRwQYOD5q6r98xqobZR3pfyM9rDRboOeP1yzASoj5+cT+GkbfjJQVJ/sl/TPlfEy6uB8k/1vik7wFxhiR3HtRLBpQ45J06xvIq/j9q94T4Z3V9G/WuWRn4ieGvBsrAHJNEPtvw3EAVvyhfO8I2RCva+/o/HvoZI47tnaRx15cvj0kShx2vtoEKTlG+dhzZiU6o5/UjGUUWnCtGItFWp5nkYclPGTJTpVa8WnoDVe/716r9jccGumXHgSJr2/Da7R23SUcNknVuljwuGBsO+Sf/seMf4Thh8J+GPSAhhvM11f7STgZUBuaYxAXbxucGqvhF+Z4QtiFa0Z7Xj4TERBlcrmJFceBxHU/tfJilrTs2/D95/xKk7Wru43vv//XYN7/mZlYniTscX/XlSo89n9IL5fk4HW9B+B6yRrWLZqn0tdu/biYxYVPtuEpRWPJTjWSm2thAP7E20CPtVtGo1Vq5W3EOQTI9Muq+/6Vqv0H+yX8c+ddV/4LiPw17gGql3vvOxUxCIA9S7tgb/9rXfkkZlY/rhDjMxynEAcVPRNrOvo+SMvyTwR9KVNeJCz6YeDp4J473BPUMV4LOvqd/NVgZ/82hryRl4qE8aZ2YsIPeXDmW3CFzGvJD5v0MT350eCx5KAw/5fPiFaj8lBwnN5SZ6mADRTLTrngys1qjFsfOUUj08sHYuGLo7uokNvJP/sm/5n9s7P97/9f+4IdJ3wOOVD6SZTifn/zqV/+ff/P3/p5TR4kilvxBPUO0n71UPuLN36GHuX/FS0IOSXZxvzf1MDDncW8laEfHfcLBe3AoKF+7sCDBIu731pdz5vQ9Xb/zzysjt3658uVf/9/+wu9juqDlp3C83KzMVLsbqDIu8oZXbj6MxKawIAbhX2tj0G95NPJP/uPKP9bJL93/sz/o/43/lPg9wPQ4JyVUAx7kH/ydv/OHdYzLK3G/N3Ufc14e51iHatQxKteTYFQeLYpP+6c87nFTJwfGFXUWlPUkxX71FcpT7iaQ+7RydeZP9T1u+p0YEpT8lMhM7TQjM9UuxOOktW7vZhIIdV//QLWyNgoDMGzIP/mPHf9p2gOstpmE5MA6RuV6kuJ/lfE/5XGPm7FMDkTFvhpjsti/0kr1vpgbzmtxitW2FsvJGg9zrrSSJFH4YxbNNT/j9IKSnxLJr2NlptoFYlb18XUnKgJxgHgc9+qVoCb/5D9N/HMP8HcP6ILBPOlhTK4ksTBIHcN5LVZydAi98FCZWA+iml9kDGePREcUb4nbfeDYzSPDeD2ISk7ReUioTXKBcL/PG6fv8lPNyky1NZ4Pj5W35JrnMimAxLjqMIQB8k/+08g/94Bg9oAQjcgrHioT60FU84vQQ8JjDzm6ePCH0AtlMO5aBuQuCpokecF147efDCzWGM7F/mxc7gHHbmqx2LUWj12I2SeZOze+Le8senhWfOPOb/mpdmWmmhrLh1n/Wqt2JcwyyN2Gut/b2ivoZwwq+Sf/ceCfe0Bwe0BIxuN51XYtA3IXBU2SzJ/EOC96eJyjzR9CLzwq/R3UKzmdNMCT7iFHtxOH+GbJKLarPB0kqdxo40Vz+2SNFBFE/H2KbfNbfsoPmakGG/y8bMibSVBJaMMgWfTbYCT/5D/q/HMPCHYPCBqiKmFX+jtIS8lpeNI95Oh2Ih3fDM3lWk/rwO00LbgfFvvfrQlNKfYvRP26obdZE9eVK6WKu777r9+tOZYsOL5xh0QqP+SngpCZMjbjSV24IolFepoBCjoYoQn3yD/5TwP/3AOC3wOCBDSXbU/rDzOZVPGnHhDetUNTVjOZaPI3/uT9d2xjEeoZaVx01YPC3ZrEwE/evxTV683mnHdqFoqck0rusoXSXXvjgCC+H58t1cg6lp8KSmZKrm+3UQnltADeQOnjfRgp5J/8J5l/7gHh7AFBQRmG73jEMaeSP3Xfdz3CNKLHn0exj9iqR3QKiW9ejUt/eAi9xzpzuBNIbNtqUP2BpCrZ8NpKUghSZgrXFGQhhrhBV9rz0zgh/+Q/ivxzDwhvDwjIUFyKtXqEj8B9q4eI1Uj3x4fFwR7bsxr34iWdQsptV4eqPO0fjtp1fqlQ6qlNfiinmjsptWqX2vaFO5HxchONJiYmWs5mDkpmamRk5G195ItKUjSZjo7BXRmy8fHxHvJP/pPIP/eAcPcAv6EGQo9HWEaq+ZNy27a3OTr8KYPwpRWWscwlFzJ0/QuW4bwRNW9zX8F5acnskLvDflmwqmBt+Oht1soEt9t4byAyU9qrFkRxhzhDx6Gq/l72sa/JP/mPDP+2V5V7QPB7gM9Gs+1VJX+H/bJg9ctGJLzNHz4dvFYbvzt4gZTB29x3RvXHvtk3t59cHYjOolC+Vpv4sU3u0Dcfb59R/bFf1TcPHF+4Q7yobJrrLW6YgchMGd6v/TSqJTScwxMTJ3WcL/qf/JP/JPHPPaA7e4CPhuE126P6R5kM+VOAaobqj30rKbC7/Ens7oblZS6SLmPRrVUUWYvCdeGJGU/OloeB3FVvKHY2uS/cifzUlniMmj5GC0pmDF60oOTLkgDVN1N+xvqSf/LfCf+4Hu4B8d4D/IBoE29YYRnkz4CHokh3+ZsoDgzZmsxJL2LSch+p/oiikkZvwRmy9TiTLmDfKtAfQWVR62Nf9XOpmdcHJTNlqATsQGqLrHsaTVr1oOKXDBv5J/9t8u/baQD3gO7uAZ1CGchDtiZz0ouYtNFHb0dKSQNe5biXjQ7FcLYrBRYHHnfdaM47xSSUDA3c02BXicqVfOHO3IiRhNXE64OSGZvzW482ibh58+ZjP72x5J/8t8m/b5qz3AO6uwf4YBAWY1k2OmR4VArsDn835ntOKANwj7HMTTxcFPuzdpXAbiYE9kxvnsjmnT3GsR0PlFG1K0SFLT8XlMyUHBPvNLtxpxlGPOlWAAYr+Sf/TfE/Njb2LveA5OwB7WI5kzmhjL89xjI3ZTRn7SqBXUkIRGnsKkPw6cAW6amzOR3Gfu9EJSEQZVGrj5zK5K7u5lJ5yy2lGkxCYFPyU0HJjBmGwAaZbqq/NrshP0b+082/+v8xMZhfcg9I1h7QLlAa2zIEyV/9BwzEfu90PSEQ1f6qvaeDj0hPfaB/LG9z16SdUOnJktIhd408Dap/qsuqln3jrhn5KS0zpTbNIT/vS33eQ/nuAlluysiZkSP6x+Sf/IfI/yu/+eceEJ09oB2g2p+VAEj+GgD9Yz1khM8fPMtVntPi4GVSUx8eBWA2u2Y058tbVYkNOYfcNYCH+L9v3B0nP2XKTOE43c/7gofRTymtpAOhEe1IhZF/8t8u/0aipq/8cw+Izh7QptG8ZRqB8DyTpfrwKAATLn+3nly9aBmAu2ktmd0svGPAw1ca6b3vXLQm/25ay6U2C6/4P7+yzI+Tn4LMVRCJWlAB0JsxGW6Jq12JLT5L/sl/HPnnHhCtPaBV/DCTuWgZgLtpLZndLLxiwENVGrlVHJypVoPoXyAtx2Pi6cBz62FjLOxrUJN9pvqYySF3TfVb6bkVA+gbd/Xkp5Rhc85vmSljM74nm/Ec2W2p37QRM0z+yX8c+eceEL09oBUog2/Gis8lf01A9dNzq9/C4w9lsi0JtSFS0szDxsDdbsc1o0RqlcxQwSF3TSBbKN0NMK7ZU37KiDn1fVFUn/lCNuoBsts8bt68ORlAXCv5J/+h8c89IHp7QItG87LlMSV/zfXb3a7FNdtVAFnQpDl8+HTgimU0vwx94lsVoChm32S/5cpXLO+Mr9zZ8lMiM7Xrp8yU9X0blBprHTrG1E8lA/JP/sPkn3tANPeAFoy/ja6FGcQYqp+uWEZzePzVxuZ+cJKUHI9f+N6Xz9qx4F0wmqvisnqmt8ldM96Zjz49a8cB+vn5tvxUEDJT1ia9h89vJHVFeKx9ExNndAU98k/+48g/94Bo7gEtGM1Vsbl/nMmQvybwg0zmrB0LHs6ioQxky/DbIx3tP3B8/ftXT3ew8WFhnWo2KQWLoyUzFAh38FzgqTxxngZrs7n63den/fx8U36qFZkpPQ6ajXuUTRnfE+TchffjShLnsPYA4lid/JP/OPHPPSB6e0Ar/MFAtgy/2NlfUtp6+I8ymTNhf7f9wKHa6cC/FKEYltHcVWF8VCG89bR/eOKTvjOtv/f9S4fvDc9TrvprrdpL//6lDibblCyyB0giUT+zjWSJsJBZC+ZGMAtLaRhxc0lbMFWfrVXJND3Y9lWqy5Cf+vNWZKaMcYC4yDkoIzR6PY7kQyhqgUSpxI0B6e9Vv6vzkX/yHwb/3AM63gNW/d4DWuFPDE7T6ItdYSIYzJKI1xP2d6vvXLX6L3i5TVtvGEmBXTWan/ZP4TpwXW0YsPN473hx8FxY12snUbZz3R6TzWxb9bzPttZkUItaUhdMO4EG/enn54v81KbB5b0OxsFzLwkrABXNJJkpSI4SazSh3/ysDEf+yX9Y/HMPiN4e0Ap/HnrDsevjbhrNdhJlKNcw8aT/ejfk5qANLQbnJqTb8Lsk1m3Ktazhd4Q7SLVCJCuuwZPsXjc80ur9Ipf3aqI48Fj9LOmEPPx/KEa+6q9qT33/dZ8nW0U2xM9s73NvoXw9DKkhLJjuE/lh1anNbL78bHB695T7pP7x9hn8HR6ObK60op/U+3LbF9zX50qP8R5X3ke91vi8Av6G96mFaxa6om4pWOMe3N/VZwTmZVDfVZ11Xr7u93covnJyLPt5s8ft9jhQ3H9h/P4KZZiRWGRs+teDUmWwjCaUfQYfEJJ/ptop+b8z8nd4SVYyb572L8jfH8t7nmfeHOHh8wryN7wPCVMY1xD1N+/jsnxGkJ7GBeln8k/+Y8U/94Do7QGt8PfDTOZ6t+XmpIQ3jM9NKFDoMAv520NcE/5PXWsR+sjyf1l4ecXTO99FT/OClUR5PfAvnXg6eKfK6Hs68DDo70ThFCTNqbbqhlMcGsS7hiHtyrfB8FXX80z9e18Zo5NHusifvH/J8JAfuEby4X2sHXrL+2fD8jZr7/at3/qgMl64Xhn79o1FmTQtNzWZlusZzWZTryvh9f3f+Q//pNrLUHoY1IKJ2C8sYFgo1Xe96CuUp9xF51DrsoAFT/3tmlrgdgwPyL5aRLOIE5PF9sXhQlWeOlxgnXfkeHGtN1e+I/F5+zr7212IC8694BbM8jz67f3v/KDylW8VK8OTH7XNXb2mFsw/NLj7n/waB+pzt3XcnPp5R8ZFkHMXRs6eGDHYLMHllPzfczGAYPRcU00nVWER3ccQyhzGms3J+zLyXhhLOLoE32uq3ckcJsHsZ95kkOM9gY0B8dKuSb+Sf/IfK/4Hp380zT2g8z1At6G7v93xGGiFv987fXra8jQ/zIQIZXS+o77zQIzfAmKEVw/nc0auB/83q4zRF2IYj8CoVv/eV60k79nqoqd53uq/4cC/VIdDvDGa+6eC/k6pprevvckwcNFcY9oKz4BCxYfF/nfxu9ZFhqH9xmjun7QN2JDDM+bc71QGczMGb4fti+rfxz7/6i89qlyd+VOtMzkV1IJpHs3he0xNSyyiWCD1USGykuXfR6Ut3UVTMrvt90NXFB4KOS4rYpF0F2C1+AYpnySLuGswh8CdX+1z6/d9I+EoyLlrH8/ju0xdzFNiJKFVVDsr/zbLm56W/zvp8f4h8VICRTGU3hIDLLAx0Oh0h/yT/8jzP3br4Ge/uVDJ5l9zD+hgD9Dt+i//Vqj8jd+8efA///RPV1Z/4ie0p3QqEyLE6MX33pZQkWf4XT3BnjLDRdTP8/r6tHdce3VhSHfRaJ6zPPUjwRt9tQU6CqEYm8X+rBtWcfid+wizMI14bTTL9cGbvHrkSTaMZh2u0UWjuVjlab77tWc+P6F+4bFRbkGM/x/9ym//i2zuU1MypxD2gokqSup71+HhcP+OZIpC6Zy9YB6+9vD/7AXTfC28EvA64IgPP4PkDotzlaf5Gw+e+exlWhK+HPEu/Fj9LLQxDsD/X1p/20GSCXRmVfu2/C3IudvIaEIlpvXMoZdkSgyjcx5GU8b4P9toMl+bFc/jJfkZGOBpVP2nlQ3IP/mPFf9Dv/o7c0cGM/eAtvcA3b5y9+mzADzNdfn7X955Z04bzNIKmRDh4amtiNF8zjSa9e8wmnVREW0kdzOmGSEjoReGkfCIUKvaQRnjUOWi/21X6/go7KLvjG00i6Sbq22pi4lEzGgOOhFQh2S8RCIJ9D/NhSyMikaNF0xnF0ds7q4Hz0CdBVM0Mfd7pjdP2Avm4X2Unh99Rs7ZUa9dUj8nA14wA00ENGWmdMU2VCBrYRx8Xs9QMrPwUQJYvifIudvIaNqVY/aMeAfrGU1n5ej9hIfRhM9/bnwGPIwoQzwZ9BwOKhGM/JP/oPnnHhC9PaAV/rTB2ZWqdodG55QYm26SoiL4rbXDuZupZzQr4/iaWbZa/XsyVYmA3VDP+MXFwVOStLchRjKSAbcOwzaOEhNfiZH8yg3lOEy400mCY55G85tQk9XQEgHfXFOlU4PdMppRrGARSQP1Eki6lTltLZirSPRArBuSQ6wF80D93+3e+85FdwHU7zn0RpTgUZCqTJtmAgaSQvQRX5DcHSanGHJD6rr9+mxdaUzLTKG8cbOyYNY48DSUbG9ZF9QTTKMHCWJI9kK86zPLaDpQ7bZqF8UImjfeXxKv4hUxrswkjtnMm2P+QKEVDlQ/kn/yHyv+uQdEbw9ohb9uq2dIgZCS6B0/k2TA5UZGs4Ru7Mh7FrVWcpeM5k3bQx680fz9q+ctT/N6GDcL43LiyeAj9X0vzMQ9xDW7ihjwPn/y/iX8XeKGF8UrPg/DWq573tRFPizUMvhIfd4S/j8ko3nf7D8Y/h1MtjGEXUCaxsyOr4er+dfnrYpGgXCHY7JsoXT3aKF54Axk844bO+QetR3GhS26ix8Wz6MFs7wFT8GbRJHDSlWyYC65x3n4P7WoVi1k+HyJbwt4wdyvrqS1ecKvz5akoiOZKUt+KnvMe28fZyhZrz8vnxvk3MU8u2v8PpB5Ez+GuYuErUUxgB4bRtOWeAt1sthJw2iCEfVQ/u+29X34/JUw5rDEBVeamXPkn/y3wr/i8G6Q/HMPiN4e0Ap/OlbYaKHYXybEIIZKxnP8NDzNCN24K/8+jd9XD+dl5o8ymQtQrpB2TV57Puxrl4TEo/7T6h5heH27Wgo6ruh230kWc9fLgDbwgm96LlTW0VzN/x/KF43Ete/gsUKBGmzG5imBVjmAZqef96I2+1O6ZG/EpkhPpjam1ctT6YW5TAhJHUH0Hfkn/5p/s8pgEPxzD4h33xkJd+GWgk4Autp3fnpL04RueemDflLu1oKJozit9Rn0fQTpoYH0l5durmzQ+/J/vj6VB+Et65LRhONYrfcb+L0E4aUl/+RfPnMuDP65B0RvD2gFXfGWJgBd9dL7GZebKqM5AtUUg4zL7QbCuv6gYgFhtMBrJUez79r/j/Ab2TQf+bz5+x6X2UWEdg9+xwOT/3TzPzExcVLzbyZtB8k/94Bo7QFtGH/hx+UmAF2NB7cVIG4/uTpASpp52OifDFt5pObJ3Mr+RSwYmWlio3Hj7PzPOkdcumyKK3U26Xd0oic8T37dj1YAqFdqmWjMl1/KE+Q/9fzfDpt/7gHR2gPaMJptBQjy11y/TXZNeQSJeJbHdI6UNGM0D7ywSmhPhn0NOsv4qAVYcjRRCyYqWlX3my/caZkpZbxca/CapuWnWjCaZmQzniW7LfGlE7ZGyD/59+Hz1sPmn3tAtPaAVoGKe5bxR/6agK5SaLTw+LPDDCAHR0qOMZhdpQ636EpXw1rsIybI+JCdxpBSrQd+H2kaMlNbjbKmW5Gfahajo6MX5TM3yXBzkFAKN8a0nqwj+Sf/LTy4XOkG/9wDorMHtGk022EG5O8Y/HEmcxIlvrsW1uKWr34ysFNlOBtSboSH0fxGT1q3te5M/iMh+DeT/8E2uWvkmSmUr1ubjC/cqc1wyZSZqjvfWpCfagXYrOvFUhINjZcV8k/+48o/94Do7AHtAAVFRPfYNJzJXwPoUt5GC58/KR4SejntuEKXzz4KaZEy4N1AX8FZsBYActdowbRKp6rWMXcjIyNvyyZYJTPVwMDxXX7KSDK6R5abMjLnmjFyyD/5jzr/3AO6vwd0AugdW0Yg+WtsNBet/gqfPyT/WUbzBqnxxmHlQre8t+GZH+yadweJH+YCAMkeslTPK7N5QvXPXnUs23bH3CGW1Etmqu4YCkB+yggPWCPTjSHePlTrqsDgIf/kP878cw/o/h7QodE8YBmB5K8OIMmnqxDqhmIroV+IxOjuVxuC/W+TIo8HjOLg5ap+ejqw1d1FwI3PqtLqvFLYJnceQKnXag9DuWPujpOZqge/5ackRhPl1yujo6NnyXZDI+eSX3Gl5J/8d5t/7gHd3QM6hcToVuk1/zCTIX8e+I+ZzGXrAaN7/KH8dFRCDqKM2lCWwUfdviaUJI3ScVNUYR9jZvNOx9wdJzNVD0HIT6Gght/KDAk1mmaln2bIP/lPAv/cA7q3B/gBZfwtdT3kIAawQ1nUw0X3+JsoDo5YIRp7E5/0nSFNRh99MnjBVs2A57nb14WSo9ZCsNf38Ta5MxfL3PaFmozpnNMxd83ITDV4r6/yU7gGraIQsepw0ZnDExNnGhUgIf/kP478cw/o3h7gkzE4YhnNe3+UyZA/AwjDsFUz4Hnu2gVJrG6pynAuDjwmVcaC+3TgeRRUM2wgTgtSQ9VxWiVyV+WJKT33O2O6WZmpeghCfgwxrfKZd8i6Z/88kv5ZJP/kPyn8cw/ozh7gFyRWt2QZzuSv+sHieddVM2qMwlpv8wFjmw/hoWcdCS9zPU8DnqgZ13YIDy1TXzwMzcpMHbOJb/gpP6ZLA8Ob5mfVuSRAVA4O0PxIACP/5D9K/HMPCH8P8NkotL3NB4xtPuobW8+6u15mDdFsXrcS3Z6RMrcC4KplNL+I0vW5ep15Z736iK5M7lwPg7NqLZgdc9eqzFQ94GjWb/kxfeyrNvOHZL+qr5+1onJA/sl/nPjnHhDuHuA3RLN53TIOyd+h0bxqxTJHhz8P+bnKrSdXL6baYC4ODNl90k2ZuboLgyU9hNZ730k1d70FZ8juk27IzNWDyE+5qgdIDvLJOLigN3QqKRwC8au6T/yoAEf+yX8U+eceEN4eEJBxaMvPwUBMNX/q/ofsPumKzFxjI7F/pdpoHlhOK2Hifd+ojvXuX4jq9WZzpZWqBTPvpJY7eF6gWVq1WBacjrlrV2aqwQb8SD7Ltxg2ZSTMi7dxniaTa0gui5EzRf7Jf5L55x4Q/B4QJJRRuGIZianlT7zvG2Z/QEEjcheKMto1ntWn/VNpJE3d+7zVF/tRjvNGCdWap+pCOZXcqfuet/pi348Yv3Zlphp83nntBfMrDnV8fPycHB0fIM415QbTdenfkh/9S/7Jf5T55x4Q/B4QsNF8ycPbnEr+1L3PW32xH9k4b2UcLtqG84dPB6+libCJp4N3akNV+mcjv1DknUWPRTNV3PXmyndqjinzji/cdSIz1eAzn/utegAdWm0spPWYXkIV9sTrOkL+yX8a+OceEOweEIKxuGgbzquZTKr4U/d8x6MPosvfL3zvy2drJOhc7ebBC2kg7MOnA1dsTWY3SfKTD05GfrH46NOztvyQq9sZ0Tguv5HNla/YepxIkEHlLB824Y5kphpsmlmtsevX50q5YL0Zr+FYOVUPvYeavFt+JX+Rf/Jv8L8ZZf65BwS3B4SBH2QyZz0k6PYiF8sbEH6YyVyxNZmRJInqiZG+cCQA1pTXfjqwlfSiJx9+/+p59wGh2mDeiZP8HpI/7NKqKBmadMH7q/nX593NoSrpw9nx60jOD5mpBhvnhnz2gI+Gw0ntGfNDmzYuQHEP1Y8vxSO47JeBQ/7Jf1A62H7zzz0gmD0gRMPxol1eG2Wjk170RN3jeTwgWPe9Exv5vVtP+4dr4puL/StIkEsiYb+4OHiqRnbvycBBlDSZm37azpeG7eMpJIkgOSKJ3A1O756yJZfgbfBLj9Mvmal6MOSnfJXTkeve9SsRKg7QiXDw3PnFFfkn/37HngfNf2+hfL02TMN5meQ9oCbxT+0B0GmO4/0oQ/G6HaKg2kskyCWRv7VM5pSd+AePM3SaY3Ujymgs1CYGDjxLmuGM0AtbOUTaWFzvSS0YhRrDOV9+lrRFE8dueCDI5l9XvpT7c9Oz4ht3fslM1UMQ8mOGEXFZCjvAkzWUVGMJHkXVd78uPO352Y/kn/z7rXISBv9p2wNqHhJ83AO6AWU0Fo5ien/iJ470m5NmOCP0wkM5BHHM8eMPxvGtJ/1LXh7npIRqIPTC9TB/f7Ay/uC/r4x//LOJKCUugvdLXh7npBzT4dhNe5i/8s1/Vfna7V+rDPzG/+lrGVm/ZaYabMyPgtqY1eeOaWMiqYaT6rffU+0LdY+f+VVlj/yT/yAfaILmP217QNJKiYvs2tLK3/27lbmf/EnToFxJSqgGQi88CrvEu5S464WtDVuQGOd4Jwci9ELdy66rjjF3reJuLLduVm79y2t/kARvOp7APRcUxLfFPDEEoRfqXnb1PcFgdjedsYnPbo7f/sc+bma3gzg69/geX+WnYOxBcgwxuIhpVe2JfH6ijuol6W3OuLe/UW2Y/JN/n4zxyTjz//Xxb/z6z09+/F8Hp1YTvweY+tRJ8aZ/3Nv73/wPX/nKj//ZxYsVjxjnWPOHctjqPnY9DObl2HvTx4uD5zwN5ycDe3GVo7tVHLhtq2SM3Rs+XHgnbv5WUjaV9wqlc16Gs5swEVMpor5c6badIY3QjK/+4qyjN094bJAU5MNmtu53klaD72pbfgpxl6JLC4/Vj9S/PzMMiaJ8/pg+qochFXdVBVFJWJH7+UvF0e+Tf/LvM/8bced/9NY3fjcNe4D2ol/97uvTmQRAxre7/vzzn/qpsodxuRdXObofZjK3PVQyKhKmkQj+XI+zZ6hGzAqgHIacDMx53cf4P/vgf/Wz5GpUIB7npVqPc7zE793jxpwz53kf6u/4f2iySsKWK7eFzaSDTSwbhMxUp98n8ZumF3HHMJDstmuOZYlx1clha3HV8cVRuSR7uTJwOnSC/JN/8l/Lf5r2gKQZzGiTX/3qCEI1PIzMWBVAkZCTOa/7wN8Tl+gopaULXgYnSm5Dqi7K1y8azGse138w8aR/UhauRRmos0niTuLbCl6LDY6zIFMU5esX/c01j+s/UIvlpPlaKWywoTeNdr1EQRSe8MOzhU1Ty2o1aNqjOObllTIW5FLcKsehuISOM4Wn0X7AJf/kn/zX8p+mPSBJBrPE0/eIwVmoY3AuQ6ouyvclGsxrHtcOj3Ni+POEyNHte3udB55FTdcYsdcw6j2v98nArikrJwtu4rzNGiJFtO+1cCKzOmqaloi7w4Lu6VnIO7v1ZOUkgWvRWHhmW/EWBVHiuEmDoKkYSlyf2jgP6myYn8vG+7LBwnzSMAoOINUVda+jcHLEKa65Hqfkn/yTf2/+RZI0MXsAjOkkG8xo5omJMjCHPXScj9Q1oqZrjNhrGPV1rncXxnQmDZACKKU6hugB1Ce6rbAB410Zy8U61+hW+kNBE48FKZHe5iPD+VD8vlRnETpA5nG3s6uxcKuFsljnGt0qTxCzb3IT0pvLSrOGQZBqBo3QSra+es03G2yaB8e9H0aElFzWx9n7aqN9GKaR0OxGIvq7msdd8NqCEUL+yT/5T+keEHODGYowFTv/QAqglOoYovDcPu62wgaMd9WKda6xIqoZieGvKUjJ7cUGRunereLgTNie50PPcv+sRzlsIxxjYB4FTeostIn2NruL5mG51cV6C5JUVpoJ2+sgXoVZrySPowW9UJ6HmH2zn6k2yotGWd0dlMT1a+MKyKPW1IZ969atf6he92NrkdW/F5r9PnWP54yiEBWJeb3jRyJVpwaEZdQdiGZuS4kiCef/by3+/4b8k/9m+E/THhBTg1m3Pa/3SMntxQZGKarrzYTteYZneTWTma2T6KeN+nkUNMmkFROfvH+pTnEQs2248dDqtX5/P2KtD+XjBh+5UngNrgPJjONP3n+niYUr0d5mjfcebF/yFoavWjxRaamA1/r9/YizQ4iF+o5HrgxSg+tAIks257S1iWGjNY+jIbtV77g2LJmpBtd67NGwJAEdmMexRttsx+CRh8UX5uegL8I+tpeY23tWglOxw6Qu8k/+yX+K94AoG8yKzz9pEJ++0ej9ygC95FUcxGobEg/tO3+ItYZ8nDLOH0EK75jrWFIGdaL46wi3n1wdqCNNZ7cSFCzwesjZ3ZjvaWmBd7WjUZSkODCkjPUFrbV8THv5YXGwp8WF+yDJ3uaqp/sHzoC3pnNNK7nZy+r1kLPrmd480doCuX3SPXYrOEN9BWfBS2fTq/yrX+VQsVkam80LL27DlJlqcJ2eSUjiBTNjNQv2Me1xnrQmPI89UB6wFm78fi+oAh/qsy+JV3HDupdlP7+T/JN/8p/uPSBiBvPJBh7moznQzGcpY3SgTqEQuyGsYw6vX8tkzqkPb4k/VPCT0Ish9RkLdbSWa0qAx64kdlhwFTaKgyMN4p3rNRi+65KwN+96pZ/2T008HXgIw1j+Dm/1Xoufu9GulrRxZDmXBu7wxK+e9kcaxLrVa1j01t1kjUJ53s3QLpSnevOlh1gU8Xd4KuS4r+nPdb0bAeiIiuxWSctVYcM2NqtQZaYaeP2u2NehrvtdQ2ZrV1c+s7Lpiz5eA1QKFgylgiNPFk5gcI042m/Vq4mNQh5Ks1KYomR9PjyMc+ApiL4l/+Sf/Kd7D4gS5IRnsYHhvNCK11cZpyMN4p3rtV0xuJGwNw+vNKTs1M+HYhgvi7d6r8XP3YirlnTogPcYxrNoO++3aOh22vbcBMDiwFAnlf0Mb/NBJ0eD8TOeN0/IwrlUL8s6qIZFFckf8EIEqblpFUaAesDdRh6eLnmbjgprSGEKHdu5CmPFei2OdEtBnIrAKIKhgRhLIza0Rg8W1wuviDxswgM2hcQyLPr4u3jw9uodQcprL4VhrJB/8k/+070HRA3KxriqE//MhnnR6mfBeyzG81IDpY2g2h4SAOGJTpzucmhHEJ98cNIN3TgMpdgJxFB+OrDlhnwUBy+3Gu7RjLcZP9PInSuKj2M7HKPlnJ1gFkqUdHXmENfW6lFfR6cihyV4C8YC9W+7ITPVYNPUJZzLZqWzBjJboYQRIbEKR+liCG0am3mzbU8MKMTP3utGshX5J//kn3tAlKBtDcXx9yzPc0cPcAinQCiGeIx3AjKUEcs8h9jmVsM9iOMWquWetxBbDHULCbnYbMMTjRCNDbegSnFwJsiiKmn1NnsvnpW3EFeGzGbRzdxs1QsBLwKO3OT9M1EQ1Jcj2V1jkfrtKPS3MiZ+Wl2LLof7V2pRHYrq2JBYy/MSDzssiUqIH72D8r74O+ZPFEs3k3/yT/7TvQd0G145VDpsA/PHr++R8I0eUbdAyMVmG57oPQnVWBZ1jtTz1xVA/g2aya5B/bR/+FZx4O5hTPPgnYkn/dfxdzf575MPQl900+5tPg6Q/oFeJhZTCOdnC6W7bjxbrnwHxVTwdyR+wGMR1XsYHR39+0YW+t94VVILeSMfNo+y1cL5jCON/BPkn3tA8mDkT3VFrQvyb9BMhkEtxVPuSkzzHfXzOv6O5D94rckW0cpTYOq9zUmFlplSP82YzWLYnjHEj0rsqL6Gf62PjFvVpyXIP0H+idjYF6lQ6iJS9iRIb3MyYcpM4RjU8PK8CivmUhbPNaM624iMvSW5trtkivwT5J9Inm2R9JoQRHqfBultTt6GWSMzJdnor4zEpeGAF84BI65yw9SmNeSntropg0X+yX/C+d8w/kb+iTDtCnqZiUQuro/obU4kr54yUziaNcsL49jU79LCksE/axzHLnodCWv5KWjnkjHyT/jPP0I0yD8RJuhlJhIN0fZEbNlBt2SSCN83zGZK1o4Yslprfp00oDyxqRVrb9rWdWr5qRWyRv4J3/nfI/9EmKCXmUjLIqu9AovsjUTw+Uh7kRq9TuR/dInf3U5L7MqR644+doX27TEPbCf18W1QpYzJP/lPG4yku0fknwgT9DITqYDhbebiFXOItqxO+DnfBPcnLdH52VZjDOU4dko8DBU5Gm4qK954YJsje+Sf8IX/ffJPhA16mYlUgd7mZEDLTGHjauN9etNbwTFrCw9cL/RxLDbPNhZayk+Rf/Lvzzp+h/wT3QC9zESqQG9z/CEeH33cmm31/ThOxbGqvH8Hx63HbNCXjNeX1KJ5uc3FlvJT5J/8+8P/Jvknwga9zERavRT0Nsf7SX/AlplqYwycNpQXXM+R13GtlBQ+8kx1slBSfor8k3/yT/5jPfboZSbSB3qbY//Q88JLZqrNzzJjFF/oTVFiJs0YyIIfGx3lp8g/+Sf/5D9+oJeZSPvCS29zDAG5wONkptrwHlzGsav2Aqk2ahz/7rZzBNxg3FF+ivyTf/JP/mMGepmJtBvNpykDFD80KzPVKuT0YcXwLKGtqk36nM/fQ/kp8k/+yT/5jxHoZSaIzNHRXMsZ2ER30KrMVKub2djY2BNz01S//45f3ixr3FF+ivyTf/JP/mMCepkJ4s0i7D71j4+P97BHIv+QcyeIhxw58n2lj33VAvlAjwsc06rf3w3Aa0H5KfJP/sk/+Y846GUmiOqFeEqOypbZG9FFpzJTDfgfNopkvNIeLBzL4njW2NzGfB53i5SfIv/kn/yT3WiDXmaCMEBvc2wWro5lpizeTxjxkdi85nFEa2/U5mtUK9qv6eB+LlN+ivyT/9TzXyL/0QW9zATh/dRPb3P0OfJNZkoWwjXtRVIb2MgxG9yQ6Y3Cca5P9/SK8lPkn/ynm398PhmO7MMavcwE4eF1oLc5wvBTZko8VjpecaPZDHYc25pxjzjW9WHTHJNN8yVZJv8E+SeiA3qZCaLxAkZvc0Thh8yUxETOGketi60etUqGvfY8YKw8xjFvBw9rJ/QG7neyEfkn/+Sf/BMdPWDRy0wQDRYwepujy0tHMlOjo6NnDQ3Wg06PeHGca1zTGjwSHXzWQ/mcBbJN/gnyT3Qf9DITRBNQE+Qevc2R46QjmSnF5RX13h2ddKU20It+XBeOdXG8q6uG4di3nc+RLH0uzuSf/JN/8h8R0MtMEM15NU7ocqrIbmaPdBedyEzJe6dkQ3I3Xb81UaW616Jx5DvbTia88Rn3yDr5J2r5h/FL/okwQC8zQbTmQZjUx27sja4/7bclMyUlcV/o41hsniGMGb05r+A4uMX7pPwU+Sf/9flfJ/9EiOOOXmaCaBaWt3mAPdI9tCMzpV5/CcewehMK68QAx77G9+606hmj/BT5J//e/Kt2m/wTYYBeZoJo33NAb3MX0Y7MFKprmR6fsBc9HP/KMfCRh6tZzxHlp8g/+a9agy/oeOFmVS7IP9Ep6GUmiDZAb3MkNs2mZaYkw96MLSx085jTiqV80czmTfkp8k/+q/pwrlnjhfwTfoBeZoLobNOmt7l7Dy1HMlPHyTlhgzGSxXZbTRgL0GNxWT94ybHtpSbeUyU/JUe+U2osXif/5D9I9ExvnnivUDr3pUKpp7fgDGXzpeGjlitl8fcrhe23e6YrgRuj4rEl/ymd/13kjF5mguhg406Ntxn3Cukj6FNLydhho2Xx9060SNvYNO9Ivy8d87oxQy91FfcQpX6VhKQjfVgcHzd47Ul1/T+pNsjP1GvR/sLwnE2laQyQ/+D4h2Hcl3Mm+wrl+d68s9yXd9ZV21Wt0nwrb6mfL3rzpYd9D5yBq9997asqhYRZkP+Uzv9ugF5mgvABSfM2YzGAMD+eqKFFjax0o6Rssw1enefq/TNYTDsta2tDpKK2GslMidRT0azMFdWsc7mfglmJzOwzeJPU9f9T9fe/atDnU2kZA+Tff/77ctsXlJE7o9paa8ZxSw3Gd+G9B9uXyD/nf9xALzNB+PT0rRfwuHqb8QQtnpuVBosgvDWbsogWsYAYbUn+vtng/WvQGPXDE6G+61ojmSlJEHulk8Tikm2OzcXYnDZxrCyeqA2jH79Q7XO/N804jQHy7w//7+Wcy9m880i8wpVwW3kLXujejz49S/45/+OwR9LLTBD+LXbDelGIi4YmFmTxAmx4LIxImBnDcRvK0rbiJcD9ywIM/dSCLKT71nesw0PfrvfBWNhv1+FiT/7/VbtldbsFqf61anCB4+VfacKzM5WWMUD+O+MfxnKbHuV91TYRsqFaEeEbRy3vLGVzpZU2DPAD97Puv36X/HP+R3i/pJeZIPyCHK/phWc4Bk/Mi9YChhKyc1jk4DkPon8k6WXOKFerk3JmW/E81JOZwnUbagrw+s83K0MVxfFk3otqf+vnphnnMUD+2+dfQjBeNGPIuq/LOZNI7ruaf31+cHq3aeMGiYBICFTG9DVlEM/2FZyXYnAf971LeB/55/yP4J5JLzNB+AnD27wRRW+zHPPNGTJH7mKFjO0wr1ceMLJGUQKdAPOwGa+Dl8yULGpr2kuCeLwkjClJtvmsyRjCqTSMAfLfOv+HBqyz0MhgzeadPXh8ewvl6z3T274bm/hMfDauA9/V2PNceljPSCf/6Z7/XeKBXmaCCGghiJy3WaS5pozjMSyYj6LwxAyvkSxIB4a3Y6zeAu4lMyVHgDoOcAOfmcAHsY42zaSMAfLfGv+uoQpPb30v7wGMWIRrQE4urPvCdynDeUR9/6tGMc+4LvLP+d9N0MtMEOEscpHwNsPjYuiAIoP8WRSPwbBwStyb3gBejI6OnvW4nyOZKXlImTUzzuN6HOsF3H+DbPkvrJ91N80kjQHy3zz/rnf5UK2ijlFaeo5wjW7fZzZXviKydp7XCa+z1n0m/+me/13aQ+llJoigEBVvs1zHnLFQLsehgpRkxesFfsdUI7Fkpq6b2qZIKEnaWIJh4LFJVhr8bSrJY4D8N88/4pD7cs5OHUN0Ff8ftftF/HM272zUu+Yvf7Rxlvynd/53A/QyE0QI6La3GUeYxhP7XtwqRckR7HOz3K2xmOL3PzcSSbagYZq0MSQyhsOSDLTW6vFsEscA+W+O/758eUwS+eyY5Q1U8YvyfSNsQ7Sia67/g3u/VyL/6Z3/XXpwoZeZIIJGN73Nok+qdTK34hzfJ0VjDnQmvPr5B3JfWqMUi+rpNIwpHDtL5vk9eKBU+y/1Ns2kjgHVyuS/Pv9jY7d+oy9XeuzpqS2U58Mobe0XUPwE0nbmPdz4xv0K+U/v/MceEKYTil5mggj3STl0b7NkI+/JAvMyCRMd1b4MzVUtJXUQRsnYGIyx8+KNKsriPpfgMfBX5L8x/z83+T++9lSiyJXvxPH+oJ4BCTrcx8D02tH8v/n18cqNX5j+F+Q/VfNf39NSWHHr9DITRPiL2lpY3majlLf7RB6Ezma3ICVkt+X+PlOL5s9wdNWMtUuq/a7plUnSGFD89xv39jIuBYTCADzI137t3/9o6O5v10jIIcku7vfWl3Pmrv6T/7vytdu/Wrk59mHly9/533F/JWhHk/10zH/sATrOWSeBBvl99DITRBcgUkgw9EpBLmDG90Dn8m6CvSqO3hA4utI3BtR9/QPVyhwD1YBRCSP5S7k/N43m9SQZlX2F8lQ292nl6syfmve42ffxNg2adO0BpTDmP73MBNG9ib4mmcuBZHdLday9IL8jYt6GvW4mhkQRHAOpNpgnaxL+cqWVVqr3xclw9gg/WYtTrDbnf/TnP73MBBEND4Dv3map7rQlnz+Xkg3iinEEOZD28cUxkN4xgNALD5WJ9SCq+UXnIaE20RHFWzj/Of993LPpZSaIbiIIb7ModGid0pU0xXiq+72tS8COj4+fS+u44hhI7xhA6IUyGHctA3IXBU2SfN9ujHPeWfTwrmc5/zn/OwW9zAQRAQThbTaehjfTOLlR/SuNmwXHAMeAqErYlf4O7JLTyTWct0/acnRuIZeUxTdz/vs//+llJojoTHDfvM2GUsYeNDnT2J/q4eOUcSx5L233zzGQ3jEAzeWa2N5c6XaauO+7//rdmtCUgrPA+c/53y7oZSaIaHkFfPE2S6WkXTHAr6W8Ty9Ln+6Pjo6eTdEDGMdASsdANue8U2Ms5py5NHKfLZTu2g8PKIrC+c/53+Zn0ctMEBFb7Dr2NmNCy8ReZI+6/VGU/iim6J45BlI6BnSxD6pHHMU3r6atPzj//Z//9DITRAQxPj7e04m32ZjYaBT2z7gSRGe1BBH6N+n3yzGQ3jHwpUKppzYBLt7FSzqFlNu2irqUhjn/Of9bAb3MBBFR3Lx5c1kmZ8tlgPUTNQs71PTLPfHgL6fgXjkGUjoG+grOS0tqbZnMu/2yYFVC3Eiqt5nz3//5Ty8zQUQYhrd5F4kMLRjbF3TsFid2NVR/nNQxfignm+AHLo6BlI6BvkL5Wm3y3/YFMq/65uPtM6o/9qv65oEzwPnP+d8M6GUmiOgvfi17m/V71M8Z9qCnp2Eq6XF+HAPpHAPwmsJ7anmZi2S86qHCVhRZ4/zn/D8O9DITRAzQqrfZyBDeacU7nbIFExnl+xLXljgJJo6B9I6B3oIzZGsyJ72ISatAfyRZSYPzP5j5Ty8zQcTMa9CMtxnlUdOqR9xinz5OqieGYyC9YwBeZZaNPh41lQJzpcec/5z/9UAvM0HECF7eZkxcPPFC01m/Tkql7uC1mOTsuYYbyyXp060k3RfHQHrHQM/05ols3tljLPPxQCltu0pgEhICOf+Dmf/0MhNE/J6KlyUT+qFob+7bOs7GIrDBHmtq0dxMmvQYx0B6xwBKY1eHHZS3yHC9B4zKW2457YQlBHL++z//6WUmiBhCTdYvy2L4hfzUrWA8DT9k8kdLDyIz0l+JOZrVY8AcF0Q6xgCq/Vlyao/IcH2gf6pLa5fnkzL/uQf4N//pZSaIeBnLZyzP8hdqEn+ujWZTgxPehaRLqfmJ0dHRi9Jf60m5J46B9I4BeJarkttyzmUyXB8eBWA2Of85/03Qy0wQ8TKYT+rjozpe5iOBdmQA6wqC7LnmIPF/rl4nKkXF/X44BtI7BnrvOxctA3A3rSWzm4VXDHiclUY4//2f//QyE0T8PAfnlWHs2May0dbldffk9zn2Wkv9uyj9NpyAe+EYSOkYUAbfTHWogbNAZpvpt9JzKw58jPOf8x+gl5kgkmk478prXsjvWfZY80AiZVJiWvUYMBVViHSMAZTJrpKaKzhDZPZ4ZAulu0mJa+Ye4O/8p5eZIGJuOOPYzctwvnHjxgkdy0aZodagCwGony8TMEY4BlI6BuwqgCxo0mS/5cpXLA/9S85/zn96mQkiwYbz+Pj4OfVzD/9GHDR7qnlIoqVbPSsB44NjIKVjwI7N7Zne5hhoAr0ffXrWjgXn/Of8p5eZIBJsON+6datX/r0XwFfCa3Ep4X26J/13OkLXNIYKkM16OSRplGOg/f7ejdIYaIV/GMiW1JyvYwBe6ySVmT7ugePqd1+f5vxP7x5ALzNBJNxw1jFZAQnaIzliPuH9uSrZ0xcjdE1TwimkBueQGd/o9bLQcwwkZAy0wj+MWsto9nUMZPOl4SRoGNeD6rO1Kqm+CDwgcP53b/7Ty0wQyTecH5nyc1wwW4OuthilqnDGpmm25+par3i9Xpda5xhIxhhohX9bbxhJgTSam4edRIn+5PxP5/xXP6/Ty0wQyTecf1cW1SBkprBgPlMNMkab8u9T8n9F1S4Yr8XvZ6Th33dVgyTeimrn5TXQjkW1OnhEnsu/h433T8l77qhm3s8luYYg+nFBL5hRNZpR1Mb4/dXY2NgIEkCNRf86x0ByxkAr/PcWyteDlJuD0ZzNl59JxcFN/HtweveUGJzFvtz2BcMALfZ9vH0GDf8WdYr1bK60cjX/2uXfLWGddwqHyYuu5FsB33H0/kJ5Cu/pzZXvmPcCDzCuwe++xndUK4+Ur3P+p3P+q/bv6GUmiIQbzmoB/X3J/n0Y0IKJ5Bh4XxDrtywLYUYWUNMrg9/PSavIe/EelPNdlNfcVm0tcxgn944sjlPG+5dkEYbQ/J68LiOLZSClYY3juEXZrLre1DUtN9Dm1hvpto57VD/vcAy0B/HSrUVpDLTC/+D0j6arPc0lX8eA62nOO7vwwCLeF55ZGMOuwamMaNMzi9/fK5TOoR2GipSG8R4pWe3y35cr3UZIBMJKsjnnHRjIMJT1+1VbgiGOJD3EG2slEDHafef/g1/7t8+/8q1iRbfhyY8WOf9Tuwd8Ri8zQSTfcM7JhJ8KaME0n/bxHfNNLJhmSdoeWRgz4lkYsT5vqs7n4Xvvyb9LhqfC7/6bO26Dilj73PodcY8rHAP+ePVix//YrYOf/eaCMlBfa61hX8fAYXjGG4+v6wmWcI1jjOYj/iWEZP3wNaXnyhgesT5vyvPz1PdmC849+b+S9lb7iZ/75d/6Iec/9wB56Pg36meBVgVBJBhqot+VBTOIyW7Hs7W7YOrfl633NFowL4tH4pL8DOqhoygL5rMIe5qxUf6l9bcdLPbQGVXt2xwD7XuaVf+9itIYaIX/oV/9nbkjg/mw+ToG7JjmDoxm93d4qqsN4/pG83s55zK80m5ohvoZBP//3a/9/u9XeZq/8eAZ539q9wAWBSKIFHiah2XCB5Gs0WjBXJNFTWOriQUTR2+Txv/NNFgw3xLvwjPD2+A7Ip4I+Hm9jXJ6evotjoFkjoFW+JfwicCq2h1jNK/BsH1jNJe3jjOaD8MvnEnD0J6pZzRL/HMJcdTa4+w3Ip4IyPmf0j2AIIiAEHLmtLlg4ueiLHJY+CpNLJhZWQRR6vma/LveggnMyueeDXCD2tRFYiJoNNfdKDkGkjsGWuE/bPWMKqNZ/USsslzDjCvZdozRnM2VsjCE+x44A+r919x/1zGaxaidde/ro08D4V/iqN9Izqlr5/zn/CcIIqEQFQ0srusBfDyOxcz4swHjdyRLPBLPAZI7kIRyWpqZkHLe+n1I3oO/zRkL5sNMbcwaJJZWAu6/fV2OPEKc3j5uo+QYSO4YaIV/xPlaVe18HQMIjaiKQVbGrv4dKhmS5LeEBD8kISLx7zBh8E1CIq7R/L234AzhPfibm+AnRrP7fituGaWuob4RVF+r69ivrqa4eYLzn/OfIIiEQk30U7JgxqEELBZEfZyHcq+vZAGth0eyGLPvOAbYdx6A/FucSkHDKNYhHVLN8BWM6Hqvd41yZZCz7zj/2XcEQaTtSRnyQThChEdkR7XHmcO4NRs4ioOO56pqJwLstyA9NBwDHAOhIIre0nqAhJzEEa/35ZwdZRA/Ruyy/TqRnNtQr1sN6n6C9tJz/nP+EwQRzQUzbjFZp5pYCM/VWUx9Q8CxgBwDHANhGc2Ri8s9DvDyHmcM4z68DGq/EHQ8OOc/5z9BEBGEUQb0CnujpY1mLMCsc44BjoFQYCtAIO6YzDbxsJFzJoNUHuH85/wnCCKaC+YMy3+2tWAuyoI5wjHAMRBjo3m2yvgLoNx0Io3mvPPC6rdJzn/Of4IgEo7R0dGLsmBusjeaA2L/dBxgEsqmcgykdwzYYQaQcSO7jSFJiAdxC2vh/Of8JwjCnyfmLTmeu8DeOB5jY2MDssmscAxwDMTbAKy85SbVmQbgg+1LZLg+egvl69aDxhrnP+c/QRApgVooH8sCcI+90dQGMyfHcnc5BjgG4n4vfQVnwTICC2S4gdGcd4pWf80kaP5PkeGmjOb5pO0BBEE0vwBclgVzjb3RGCgYIJW2KiMjI29zDHAMxN5oRoU9wwiEXBtZ9gZUO1T/7FXHM29f4Pzn/CcIIiWQ+Kw9LAKjo6Nn2SP1ofrokmwuGxwDHAPJMATdGN0qvWZoIpPpWqC4SrWXubzF+c/5TxBE+haCBYlpm2RvNOynWemnGY4BjoGk3JNbzjphIQeB9JMVyoKqg5z/nP8EQaQMagG4pjOoY1AZqitAljRKpkos27scAxwDSbkvZfyNWMbgXt/H21QFMA3m3PaFGtUMKevN+c/5TxBE+p6g12TRvMPe8OyfR9I/ixwDHANJui/E6kJurjpWt/SYjBtGc770PGmqGZz/nP8EQbQJXRYUT9I3btw4xR55AyR8qH45QEty8gfHQHrHgO1thleVsc2H8NCzTpSXmfOf858giPaepF/I0dND9sYb3Lx585lsJnMcAxwDSbw/V7M576xXh2mUn5F5N+Z71TKaX3D+c/4TBMGF4YIsDPvMoj4EYtd0n6Sh+hPHQHrHgC0/h9Z737mYZu57C86Q3SdJkJnj/Of8JwjCnwVCC7fPszfcTWQ5bcL/HAPpHQPZXGmlymjOO8tp5R3ed+hWVxnMBWeB85/znyAIwsX4+Pg5PFEjdgsxbilfLK/LYllKU4wfx0B6xwDKaNd4VgvlVBoL6r7nrb7YT0OcN+c/9wCCIFpbKGb0QpHWIzo5ptwTj8sIxwDHQGqMxbyz6GE4X0sT97258p2aUJW8M8v5z/lPEARRBSkV+lyXVp2YmDiZpvsXPc6tNCd+cAykdwz0fvTpWVuCztVuTnAsr4lsrnzF1mRGkiSqJ3L+c/4TBEF4LRon1WKxnjZdSgj7j42NvZSqT8vYPDgGOAbSxj0SAO3y2igbnfSiJ1fzr8+7DwhViX/OThrl9zj/uQcQBNECRJtyN01JEDoJBpWxVDvNMcAxkFbuewvl67VhGs5LJMgl8X4Hp3dP1ST+5Z0D6DRz/nP+EwRBNLOAXBZRd8R1DSX8Xu/KYrk3Pj7+DtnnGEg798poLNiGM/Sbk2Y4I/TCVg4R7/oY5z/nP0EQRNNQC8iYXkiSumjevHlzUld8Ui1L1jkGyPpR0ZOlGsMZBmZCQjUQemEXdmEpcc5/sk4QRCeL5qwsmok6ppOElzl9b1g4yTbHANk2Deftk55GJWKcY54ciHLY6l52PZQylpMahsL5z/lPEER43oYDnRgS94xqyZBeMTwoA2SZY4AsexiXhdI5L8PZTZqLqRxdX65020Mlw/WiX/3ua8aycv4TBEF0Bolv04kha3HV8ES8miR64D62oMlJdjkGyG59iMd5qdbjHK8CKG7ISc6Z87wP9Xd6mDn/CYIgfINkVGspolLcqkapxfGaFq2HlwHeBrLKMUBWmzQ4PZIDdUgDpOqifP2iwbzmcf0HymDmsTznP0EQhP8QDU8tfn8AmZ6oexzUdZ7HkaKOXcM1U4OTY4BjoA3jM18artVxfqOuETVdY8Rew6j39C7nnV0Y02SV858gCCIwYLGRcqv7sgjtq0Xo4Y0bN05FbHE/I9qbOhZvl8keHAMcA51BCqCU6hiiB1Cf6LbCBox3ZSwX61yjW+kPBU3IJuc/QRBEKBgfHz9nCMJXJN7tDiordfO6sHBbC/qBZIAzyYdjgGPAD8P5sOT2Yj2jVKrrzYTteRbP8qxXot+RUV8oz6OgCVnk/CcIgggdSKRQi9ELY+HcxNN82Ed2Em93T7Ud41qK+DtZ4hggS/7jvQfbl7yLg1QZ0Ki2V8Br/f5+xFpDPk59xyNXCq/BdSCZMZtzWLiC858gCCISXoceZFUbi1VFfr8XVIay+uxL4lHYML9X/W2ZWdEcAxwD4aDvgTPgrelc00qugoV6PeTseqY3W/JIQsnDDb0oOEN9BWfBS2vZqwR4mktic/4TBEFE2+uADOUFI0v5yPuAIzL1/1dwrNfqER4SUMSTkBVR+pL1+fAuzEEaiSxwDJCFcAGvbzbvjDSId67XYPiuuwl7hfK8q9JRKE/15ksPYRjj7/BWS8hH05/rerhjqiXN+c/5TxBEyoAFEYuXWiAfQw/TWtzMGLh1eAUkNq6AqlNIKsGii7+LB2Gvzvs35LWXmA3NMcAxEAXjefOEGM9L9ZQ2gmowrJEACE80dZc5/wmCIGKL0dHRizhGk0Vw00jSaLbtyeKJ2Ll7EKlnr3IMsFejbEBvn3RDNxBKkXN2gjGWUdbbmUNsc6vhHgTnP0EQRJw8EaegoSmxcMNjY2N34WVABrZaWK/j7ziOi3vZVoJjgAZ05S3EFkPdQrSTN1v1RMOTjLALef9M1IuqEJz/BEEQBEEQvgDyb9BMhkGN4inZQumuG9OcK9/pLZSv4+9I/oPXmr1FEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBEARBJAT/PyrGPdMf7PGgAAAAAElFTkSuQmCC)\n",
    "\n",
    "Note, that we can now label the edges of this trellis with the following probabilities:\n",
    "\n",
    "- $P_{\\text start}(c_k \\,|\\, \\text{start})$ on the three edges from \"start\"\n",
    "- $P_{\\text stop}(\\text{stop} \\,|\\, c_k)$ on the three edges leading to \"stop\"\n",
    "- $P_{\\text trans}(c_k \\,|\\, c_{l})$ on each remaining edge from state $c_l$ to $c_k$\n",
    "- $P_{\\text emiss}(o \\,|\\, c_k)$ from each state $c_k$, to an observation $o$ made from that state (not shown here)\n",
    "\n",
    "Do you see that our trellis nicely shows the independence assumptions of the HMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8qPpkVjiYaU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64d5cd7e9c55167a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### A Naive approach to get the best sequence\n",
    "\n",
    "To see why the Viterbi algorithm is so useful, we can consider another way to calculate $s^*$:\n",
    "\n",
    "- Iterate over all possible state sequences (all ways to go from `start` to `stop`)\n",
    "    - Calculate the probability for that sequence\n",
    "    - Store the highest probability seen so far and its sequence\n",
    "- Return the sequence that had the maximum probability\n",
    "\n",
    "The problem with this approach is that there are a lot of possible sequences!\n",
    "\n",
    "# Exercise 2: How many possible state sequences are there? (5 points)\n",
    "\n",
    "*This is a theoretical question.* Assume that you have a set $\\Lambda$ of possible states (so there are $|\\Lambda|$ states), and that the observation sequence is of length $N$. Assume there is a transition from any state to any other state.\n",
    "Write down the formula that gives the number of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTX85IZHiYaU",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a3f990e075d01747",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<font color=\"red\">YOUR ANSWER HERE</font> \\\\\n",
    "Number of sequences = $|\\Lambda|^{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "CxfZN2UxiYaV",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58d4f2277d79da32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### The Viterbi algorithm\n",
    "\n",
    "*We use a slightly different notation here compared to the lecture.*\n",
    "\n",
    "So how do we find the path with the highest score? The idea is that we can use our trellis to represent an **exponential number of paths**. Since we are only interested in the highest-scoring path, for every state at every time step, we only need to keep track of the **highest** probability that can lead us to that state. We can disregard any other paths that lead to that state, since they will for sure not be part of the highest-scoring path.\n",
    "\n",
    "Viterbi uses **dynamic programming**. Here, that means that we will re-use probabilities that we have already computed, so we never have to compute the score for the same sub-problem multiple times.\n",
    "\n",
    "Let's start at the beginning.\n",
    "\n",
    "For the first time step, the **viterbi score** is the transition probability of reaching a state $c_k$ from \"start\", times the probability of emitting the first observation $o_1$ from that state:\n",
    "\n",
    "$$\\text{viterbi}(1, c_k) = P_{\\text start}( c_k \\,|\\, \\text{start}) \\times P_{\\text emiss}(o_1 \\,|\\, c_k)$$\n",
    "\n",
    "So, the Viterbi trellis represents the path with maximum probability in position $i$ when we are in state $y_i$ having observed $o_1, o_2, \\dots, o_i$, the observations up to and including that point.\n",
    "\n",
    "Now that we have the viterbi scores for all states of the first time step in our trellis, we can use the following **recursive formula** to get the scores for all other states, one time step at a time:\n",
    "\n",
    "$$\\text{viterbi}(i, c_k) = \\big( \\max_{c_l \\in \\Lambda} P_{\\text trans}(c_k | c_l) \\times \\text{viterbi}(i-1, c_l) \\big) \\times P_{\\text emiss}(o_i \\,|\\, c_k)$$\n",
    "\n",
    "Finally, for our final state \"stop\" we need to do something special, since there is no observation there:\n",
    "\n",
    "$$\\text{viterbi}(N+1, \\text{stop}) = \\max_{c_l \\in \\Lambda} P_{\\text stop}(\\text{stop} \\,|\\, c_l) \\times \\text{viterbi}(i-1, c_l)  $$\n",
    "\n",
    "This is all we need to know what probability the highest scoring path has! Do you see how the dynamic programming helps us to solve this task efficiently?\n",
    "\n",
    "#### How did we get here?\n",
    "\n",
    "Once we reach the \"stop\" state we know the maximum probability, but we forgot how we got there! If you don't see this immediately, remember that, whenever we computed the viterbi score for a state, we took the maximum over all previous states' viterbi scores, times the transition from those states. But we didn't keep track of which state was actually selected in that \"max\" operation. So now that we are in \"stop\", we don't know how we got there.\n",
    "\n",
    "To solve this, we will use **backpointers**. Whenever we do a $\\max$, we store what state was selected by that max (i.e. the $\\arg\\max$):\n",
    "\n",
    "$$\\text{backtrack}(i, c_k) = \\arg\\max_{c_l \\in \\Lambda} P_{\\text trans}(c_k | c_l) \\times \\text{viterbi}(i-1, c_l)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "AGEiDOw0iYaW",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61fe883db37093ca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Log probabilities\n",
    "\n",
    "Now you know enough to implement Viterbi! But before we start.. Because probabilities tend to get rather small when multiplying, causing numerical instabilities, we will use **log probabilities**. This means that, instead of multiplying, we can now **sum** probabilities, because:\n",
    "\n",
    "$$ \\log(uv) = \\log u + \\log v$$\n",
    "\n",
    "To get the probability of a  path trough our trellis from \"start\" to \"stop\", we can just **sum** the log-probabilities that we encounter. So, finding the best (\"Viterbi\") path means finding the path with the **highest score**.\n",
    "\n",
    "# Exercise 3: convert all probabilities to log-probabilities (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "flA1qns5iYaX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "50701256-0700-4946-afdb-c76455366ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " [0.33333333 0.33333333 0.33333333] [[0.2  0.5  0.5 ]\n",
      " [0.4  0.   0.5 ]\n",
      " [0.   0.25 0.  ]] [0.4  0.25 0.  ] [[0.4  0.   0.  ]\n",
      " [0.2  0.75 1.  ]\n",
      " [0.4  0.25 0.  ]]\n",
      "After:\n",
      " [-1.09861229 -1.09861229 -1.09861229] [[-1.60943791 -0.69314718 -0.69314718]\n",
      " [-0.91629073        -inf -0.69314718]\n",
      " [       -inf -1.38629436        -inf]] [-0.91629073 -1.38629436        -inf] [[-0.91629073        -inf        -inf]\n",
      " [-1.60943791 -0.28768207  0.        ]\n",
      " [-0.91629073 -1.38629436        -inf]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frans\\AppData\\Local\\Temp/ipykernel_12748/3924563569.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  p_trans = np.log(p_trans)\n",
      "C:\\Users\\frans\\AppData\\Local\\Temp/ipykernel_12748/3924563569.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  p_stop = np.log(p_stop)\n",
      "C:\\Users\\frans\\AppData\\Local\\Temp/ipykernel_12748/3924563569.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  p_emiss = np.log(p_emiss)\n"
     ]
    }
   ],
   "source": [
    "# Note: only run this cell once; otherwise you get NaN values.\n",
    "# If you get NaN values, try running all cells above first.\n",
    "\n",
    "def convert_to_log(p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Convert all probabilities to log-probabilities\n",
    "    \n",
    "    Important: only run this function with normal probabilities as input! \n",
    "    If you run this twice, things will break.\n",
    "    \"\"\"\n",
    "\n",
    "    p_start = np.log(p_start)\n",
    "    p_trans = np.log(p_trans)\n",
    "    p_stop = np.log(p_stop)\n",
    "    p_emiss = np.log(p_emiss)\n",
    "\n",
    "    #raise NotImplementedError()\n",
    "    return p_start, p_trans, p_stop, p_emiss\n",
    "\n",
    "\n",
    "print(\"Before:\\n\", p_start, p_trans, p_stop, p_emiss)\n",
    "\n",
    "# do the conversion using your function\n",
    "p_start, p_trans, p_stop, p_emiss = \\\n",
    "    convert_to_log(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "\n",
    "print(\"After:\\n\", p_start, p_trans, p_stop, p_emiss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHdqKIKgiYaY",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd2a6b04cf0668e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Smoothing\n",
    "\n",
    "Oops! We got a big red warning! What happened? \n",
    "\n",
    "Some probabilities were 0.0, and the log function is not defined for zero, resulting in a **warning**.\n",
    "\n",
    "To prevent the error, we can add a small **smoothing** value to our **counts**, so that we never have a probability of zero.\n",
    "\n",
    "To make things easier, we define a `normalize_all` function below that does all the normalization again,\n",
    "but now adding a small value to all the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Jr6sZmlpiYaY",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d511810070a63ddd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize(x, smoothing=0.1, axis=0):\n",
    "    smoothed = x + smoothing\n",
    "    return smoothed / smoothed.sum(axis)\n",
    "\n",
    "def normalize_all(counts_start, counts_trans, counts_stop, counts_emiss, smoothing=0.1):\n",
    "    \"\"\"Normalize all counts to probabilities, optionally with smoothing.\"\"\"\n",
    "    p_start = normalize(counts_start, smoothing=smoothing)\n",
    "    p_emiss = normalize(counts_emiss, smoothing=smoothing)\n",
    "    \n",
    "    counts_trans_smoothed = counts_trans + smoothing\n",
    "    counts_stop_smoothed = counts_stop + smoothing\n",
    "    total_trans_stop = counts_trans_smoothed.sum(0) + counts_stop_smoothed\n",
    "    p_trans = counts_trans_smoothed / total_trans_stop\n",
    "    p_stop = counts_stop_smoothed / total_trans_stop\n",
    "    \n",
    "    return p_start, p_trans, p_stop, p_emiss\n",
    "\n",
    "\n",
    "# normalize with smoothing\n",
    "smoothing = 0.1\n",
    "p_start, p_trans, p_stop, p_emiss = normalize_all(\n",
    "    counts_start, counts_trans, counts_stop, counts_emiss, smoothing=smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mlDQ-AhJiYaY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51591749-8ab1-43d6-82e3-409dc7973908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed probabilities:\n",
      " [0.33333333 0.33333333 0.33333333] [[0.2037037  0.47727273 0.45833333]\n",
      " [0.38888889 0.02272727 0.45833333]\n",
      " [0.01851852 0.25       0.04166667]] [0.38888889 0.25       0.04166667] [[0.39622642 0.02325581 0.04347826]\n",
      " [0.20754717 0.72093023 0.91304348]\n",
      " [0.39622642 0.25581395 0.04347826]]\n",
      "Smoothed log-probabilities:\n",
      " [-1.09861229 -1.09861229 -1.09861229] [[-1.59108877 -0.7396672  -0.78015856]\n",
      " [-0.94446161 -3.78418963 -0.78015856]\n",
      " [-3.98898405 -1.38629436 -3.17805383]] [-0.94446161 -1.38629436 -3.17805383] [[-0.92576948 -3.76120012 -3.13549422]\n",
      " [-1.57239664 -0.32721291 -0.09097178]\n",
      " [-0.92576948 -1.36330484 -3.13549422]]\n"
     ]
    }
   ],
   "source": [
    "# convert to log-probabilities\n",
    "print(\"Smoothed probabilities:\\n\", p_start, p_trans, p_stop, p_emiss)\n",
    "\n",
    "p_start, p_trans, p_stop, p_emiss = convert_to_log(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "print(\"Smoothed log-probabilities:\\n\", p_start, p_trans, p_stop, p_emiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iJMxcsANiYaZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11sJAepliYaa",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6f521724c6db98b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: implement the Viterbi algorithm (40 points)\n",
    "\n",
    "You will now implement the Viterbi algorithm. Complete the function `viterbi(sequence, p_start, p_trans, p_emiss, p_stop)` below.\n",
    "\n",
    "**Input:** sequence ($o_1, ..., o_N$), $P_\\text{start}$, $P_\\text{trans}$, $P_\\text{emiss}$, $P_\\text{stop}$\n",
    "\n",
    "*Forward pass: compute the best path for every end state*\n",
    "\n",
    "- set $\\text{viterbi}(1, c_k)$ for each $c_k$\n",
    "- for $i=2$ to $N$, and for each $c_k$, set $\\text{viterbi}(i, c_k$) and $\\text{backtrack}(i, c_k)$\n",
    "- $\\text{max_prob} = \\max_{c_l} P_{\\text{stop}}(\\text{stop} \\,|\\, c_l) \\times viterbi(N, c_l)$\n",
    "\n",
    "*Backward pass: backtrack to get most likely path*\n",
    "- $\\hat{s}_N = \\arg\\max_{c_l} P_\\text{stop}(\\text{stop} \\,|\\, c_l) \\times viterbi(N, c_l)$\n",
    "- for $i = N-1$ to $1$: $\\hat{s}_i = \\text{backtrack}(i+1, \\hat{s}_{i+1})$\n",
    "\n",
    "**Output:** max_prob, Viterbi path $\\hat{s}_1, \\hat{s}_2, \\dots, \\hat{s}_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def backward_pass(max_state_id, backpointers):\n",
    "    state_list = []\n",
    "    state_list.append(max_state_id)\n",
    "    cur_id = max_state_id\n",
    "\n",
    "    # Walk backwards through back pointers\n",
    "    # and return state combination that\n",
    "    # leads to max viterbi path probability\n",
    "    for pointer in reversed(backpointers):\n",
    "        val = pointer[cur_id]\n",
    "        cur_id = val\n",
    "\n",
    "        # Break if not set, i.e. first row\n",
    "        # that has no previous states\n",
    "        if cur_id == -1:\n",
    "            break\n",
    "\n",
    "        state_list.append(cur_id)\n",
    "\n",
    "    return state_list[::-1] # Inverse because we worked backwards"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "6AoL2DI2iYaa"
   },
   "outputs": [],
   "source": [
    "def viterbi(sequence, p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Compute the Viterbi sequence. \n",
    "    Note: you have to use log-probabilities!\n",
    "    \n",
    "    The return value should be a tuple (max_prob, list_of_viterbi_states)\n",
    "    Return:\n",
    "      - best_score (float) the log-probability of the best path\n",
    "      - best_path (int list) the best path as a list of state IDs\n",
    "    \"\"\"\n",
    "    # States: Hungry, Bored, Happy\n",
    "    # Observations: Sleep, Cry, Laugh\n",
    "\n",
    "    length = len(sequence)\n",
    "    num_states = len(p_start)\n",
    "    backpointers = -np.ones([length, num_states], dtype=int)\n",
    "\n",
    "    \n",
    "    # trellis to store Viterbi scores\n",
    "    # we store -inf as our initial scores since log(0)=-inf\n",
    "    # trellis = [#row][#column], value\n",
    "    trellis = np.full([length, num_states], -np.inf)\n",
    "\n",
    "    # Fill in viterbi path prob for the first four\n",
    "    # states (that have no previous state..)\n",
    "    for i in range(num_states):\n",
    "        targetObservation = obs2i[sequence[0]]\n",
    "        trellis[0, i] = p_start[i] * p_emiss[targetObservation, i]\n",
    "        backpointers[0, i] = -1\n",
    "        #print(\"Target observation: {} with ID: {}\".format(sequence[0], targetObservation))\n",
    "\n",
    "    # Calculate all except first and last column\n",
    "    for timestep in range(1, length): # For all sequence elements except first and last\n",
    "        observation_id = obs2i[sequence[timestep]] # Get id of\n",
    "        #print(\"Observation ID: {}, {}\".format(observation_id, sequence[seq_element]))\n",
    "\n",
    "        for state_to in range(num_states): # For all states in that sequence\n",
    "            prob_list = [] # All incoming probabilities for this state\n",
    "            for state_from in range(num_states):  # For all states that point to that state\n",
    "                trans_prob = p_trans[state_to, state_from]  # Trans probability\n",
    "                prev_prob = trellis[timestep-1, state_from] # Previous Viterbi path prob\n",
    "                prob_list.append(trans_prob * prev_prob)\n",
    "                # print(\"Timestep {}, Trans to {} from {} has prob: {}, prev_prob is {}, final prob is: {}\".format(seq_element, state_to, state_from, trans_prob, prev_prob, trans_prob * prev_prob))\n",
    "\n",
    "            # Find the maximum prob (viterbi path * transition prob) of all incoming states\n",
    "            # and save that prob times the emission chance as viterbi value\n",
    "            max_prob = max(prob_list)\n",
    "            trellis[timestep, state_to] = max_prob * p_emiss[observation_id, state_to]\n",
    "\n",
    "            # Save back pointer by fetching\n",
    "            # ID of that highest prob value\n",
    "            max_state_id = prob_list.index(max_prob) # Get index (state) of max val\n",
    "            backpointers[timestep, state_to] = max_state_id # Save that value as back pointer\n",
    "\n",
    "    # Multiply final timestep\n",
    "    # viterbi values by their corresponding\n",
    "    # end state probability\n",
    "    final_prob_list = []\n",
    "    for state in range(num_states):\n",
    "        final_prob = trellis[length-1, state] * p_stop[state]\n",
    "        final_prob_list.append(final_prob)\n",
    "\n",
    "    print(trellis)\n",
    "    print(backpointers)\n",
    "\n",
    "    # Return highest path probability\n",
    "    # and its corresponding state\n",
    "    max_path_prob = max(final_prob_list)\n",
    "    max_state_id = final_prob_list.index(max_path_prob)\n",
    "\n",
    "    best_score = max_path_prob # Highest probability path\n",
    "    best_path = backward_pass(max_state_id, backpointers) # Returns list of states\n",
    "\n",
    "\n",
    "\n",
    "    return best_score, best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFKQ-RPEiYab"
   },
   "source": [
    "#### Trying out Viterbi\n",
    "\n",
    "Once you have implemented the Viterbi algorithm, try it out on the following sequence.\n",
    "\n",
    "Note: to get all points, make sure that the cell below runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TwfGXoDriYab",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b2cd2456-68ee-4bc8-9425-8988b475523f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test sequence: ['sleep', 'cry', 'laugh', 'cry']\n",
      "[[1.01706172 1.49774345 3.44469248]\n",
      " [1.74195085 0.31431279 0.18888591]\n",
      " [0.13642228 0.55425407 1.36622904]\n",
      " [0.34130435 0.04215995 0.0495056 ]]\n",
      "[[-1 -1 -1]\n",
      " [ 1  0  1]\n",
      " [ 2  2  1]\n",
      " [ 0  0  0]]\n",
      "Best score: -0.05844609532845268\n",
      "Best path: [1, 2, 0, 1]\n",
      "['bored', 'hungry', 'happy', 'bored']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test out your Viterbi-algorithm here.\"\"\"\n",
    "#print(state2i) # States (happy, bored, hungry)\n",
    "#print(obs2i) # Observations (laugh, cry, sleep)\n",
    "\n",
    "#p_start[target] chance of state being start state\n",
    "#p_trans[target][given] chance of trans to state given cur state\n",
    "#p_stop[target state] chance of being end state\n",
    "#p_emiss[target obs][given state] chance of obs given state\n",
    "\n",
    "\n",
    "test_sequence = test_set[0]\n",
    "print(\"\\ntest sequence: {}\".format(test_sequence))\n",
    "best_score, best_path = viterbi(test_sequence, p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "\n",
    "\n",
    "print(\"Best score: {}\".format(best_score))\n",
    "print(\"Best path: {}\".format(best_path))\n",
    "\n",
    "i2state = {v : k for k, v in state2i.items()}\n",
    "print([i2state[i] for i in best_path])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbnGJ3sBiYac",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6626d09623968520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have reached the end of lab 2.\n",
    "\n",
    "If you want an additional challenge,  go ahead and try the following sections. Otherwise, you're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts_epCVhiYac",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4caecec1fc7924c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Extra: Posterior Decoding\n",
    "\n",
    "What if we don't care about the global sequence of hidden states, but more about getting **individual** states right? \n",
    "We now aim to find the state with the **highest state posterior** for each position. So what is a state posterior? It is defined as:\n",
    "\n",
    "$$ P( s_i \\,|\\, o_1, o_2, \\dots, o_N) $$\n",
    "\n",
    "It gives the probability, with observation sequence $o_1, o_2, \\dots, o_N$, that the i'th hidden state was $s_i$. \n",
    "\n",
    "The best state $s_i^*$ for that position is then:\n",
    "\n",
    "$$ s_i^* = \\arg\\max_{s_i \\in \\Lambda} P( s_i \\,|\\, o_1, o_2, \\dots, o_N) $$\n",
    "\n",
    "If we calculate this for each position, we are performing posterior decoding. Do you see the difference with Viterbi? Now we choose the hidden states **independenly**, based on the observations $o = o_1, o_2, \\dots, o_N$, one position at a time. \n",
    "\n",
    "#### How to calculate a state posterior?\n",
    "\n",
    "Good question. Calculating the state posterior is not so easy. Remember that we are interested in one specific time step. To choose the best state for this time step, we need to take into account all paths that lead there. You will soon see why. \n",
    "\n",
    "First, let's define a few useful terms.\n",
    "\n",
    "**Sequence posterior**\n",
    "$$ P( s = s_1, s_2, \\dots, s_N \\,|\\, o = o_1, o_2, \\dots, o_N)  = \\frac{P(s, o)}{P(o)}$$\n",
    "\n",
    "To compute this, we need the **likelihood**:\n",
    "\n",
    "$$ P( o = o_1, o_2, \\dots, o_N ) = \\sum_{s} P(o, s)$$\n",
    "\n",
    "To calculate the likelihood, we thus need to sum over **all possible state sequences s**!\n",
    "\n",
    "Since the number of state sequences can grow so quickly, we will do something smarter than summing over all of them.\n",
    "(We used a similar trick with the Viterbi-algorithm!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy6m-hrviYac",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a92a96f1a873e31",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Forward and Backward probabilities\n",
    "\n",
    "We can use compute **forward** and **backward** probabilities, which help us compute the likelihood in **linear time**: O(N).\n",
    "\n",
    "Let's first see how they are computed, before we will use them to calculate the likelihood (and finally the state posterior).\n",
    "\n",
    "The forward probability for a position i and a state $c_k$, gives us the probability of being in that state and that position, having observed $o_1, o_2, \\dots, o_i$:\n",
    "\n",
    "$$\\text{forward}(i, c_k) = P(c_k, o_1, o_2, \\dots, o_i)$$\n",
    "\n",
    "Because of the independence assumptions in the HMM, we can calculate the forward probability of position $i$ using the forward probabilities of each state in position $i-1$:\n",
    "\n",
    "$$\\text{forward}(i, c_k) = \\Big( \\sum_{c_l \\in \\Lambda} P_{\\text trans}(c_k \\,|\\, c_l) \\times \\text{forward}(i-1, c_l) \\Big) \\times P_{\\text emiss}(o_i \\,|\\, c_k)  $$\n",
    "\n",
    "And our special cases:\n",
    "\n",
    "$$\\text{forward}(1, c_k) =P_{\\text start}(c_k \\,|\\, \\text{start}) \\times P_{\\text emiss} (o_1 \\, |\\, c_k)$$\n",
    "$$\\text{forward}(N+1, \\text{stop}) = \\sum_{c_l \\in \\Lambda} P_{\\text trans}(\\text{stop} \\,|\\, c_l) \\times \\text{forward}(N, c_l) $$\n",
    "\n",
    "Did you notice that this is almost exactly the same as the Viterbi scores? Instead of taking the maximum, we are now **summing**!\n",
    "\n",
    "**Warning: you cannot \"just sum\" two probabilities in the log-domain!**\n",
    "A brute-force way to do this correctly would be to do $\\log(\\exp(a) + \\exp(b)$, i.e. convert the probabilities back and then sum, and then convert them to the log domain again. But this exposes us to numeric instabilities again! So a better way to do it is using $$\\log(\\exp(a) + \\exp(b)) = a + \\log(1 + \\exp(b − a)) \\qquad \\text{for } a < b$$\n",
    "\n",
    "We are providing a function `logsum()` for you that sums a list of values in the log-domain (using the above strategy).\n",
    "\n",
    "Now that we can compute forward probabilities: did you notice that $\\text{forward}(N+1, \\text{stop})$ gives us the **likelihood**, i.e. $P(o_1, o_2, \\dots, o_N)$? Take a moment to see why.\n",
    "\n",
    "Sadly, this is still not enough to calculate the state posteror. (You probably guessed, since this is calleed the Forward-**Backward** algorithm!). Right now we know the probability of being in state $s_i$ having observed $o_1, o_2, \\dots, o_i$, but we do **not** know the probability of being in that state knowing $o_{i+1}, \\dots, o_N$. This is what the backward probability gives us:\n",
    "\n",
    "$$\\text{backward}(i, c_l) = \\sum_{c_k \\in \\Lambda} P_\\text{trans}(c_k \\,|\\, c_l) \\times \\text{backward}(i+1,c_k) \\times P_\\text{emiss}(o_{i+1} \\,|\\, c_k)$$\n",
    "\n",
    "And our special cases:\n",
    "\n",
    "$$\\text{backward}(N, c_l) = P_\\text{stop}( \\text{stop} \\,|\\, c_l)$$\n",
    "$$\\text{backward}(0, \\text{start}) = \\sum_{c_k \\in \\Lambda} P_{\\text start}(c_k| \\text{start}) \\times \\text{backward}(1, c_k) \\times P_{\\text emiss}(o_1 \\,|\\, c_k)$$\n",
    "\n",
    "Can you see that we can also calculate the likelihood using just backward probabilities? You can find it in $\\text{backward}(0, \\text{start})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXvZqWk_iYad",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f68826be76ed9405",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, implement the forward and backward algorithms.\n",
    "\n",
    "The forward function should return two values:\n",
    "\n",
    "- all forward scores for the trellis (length x num_states)\n",
    "- the forward score of the stop state (not part of the trellis)\n",
    "\n",
    "The backward function should similarly return two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TpZBfkF9iYae"
   },
   "outputs": [],
   "source": [
    "def logsum(logv):\n",
    "    \"\"\"Sum of probabilities in log-domain.\"\"\"\n",
    "    res = -np.inf\n",
    "    for val in logv:\n",
    "        res = logsum_pair(res, val)\n",
    "    return res\n",
    "\n",
    "def logsum_pair(logx, logy):\n",
    "    \"\"\"\n",
    "    Return log(x+y), avoiding arithmetic underflow/overflow.\n",
    "    \"\"\"\n",
    "    if logx == -np.inf:\n",
    "        return logy\n",
    "    elif logx > logy:\n",
    "        return logx + np.log1p(np.exp(logy-logx))\n",
    "    else:\n",
    "        return logy + np.log1p(np.exp(logx-logy))\n",
    "\n",
    "\n",
    "def forward(sequence, p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Compute Forward probabilities.\n",
    "    Note: all probabilities should be log-probabilities.\n",
    "    \n",
    "    Return:\n",
    "      - trellis with forward probabilities, excluding the \"stop\" cell\n",
    "      - the forward probability of the stop cell (this is the log-likelihood!)\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(sequence)\n",
    "    num_states = len(p_start)\n",
    "    \n",
    "    # trellis to store forward probabilities\n",
    "    trellis = np.full([length, num_states], -np.inf)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return trellis, log_likelihood\n",
    "\n",
    "\n",
    "def backward(sequence, p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
    "    \"\"\"\n",
    "    Compute Backward probabilities.\n",
    "    Note: all probabilities should be log-probabilities.\n",
    "    \n",
    "    Return:\n",
    "      - trellis with backward probabilities, excluding the \"start\" cell\n",
    "      - the forward probability of the start cell (this is ALSO the log-likelihood!)\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(sequence)\n",
    "    num_states = len(p_start)\n",
    "    \n",
    "    # trellis to store forward probabilities\n",
    "    trellis = np.full([length, num_states], -np.inf)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return trellis, log_likelihood\n",
    "\n",
    "\n",
    "def forward_backward(sequence):\n",
    "    \"\"\"\n",
    "    Compute forward and backward probabilities.\n",
    "    Return:\n",
    "    - fw_trellis\n",
    "    - fw_log_likelihood (the value of the \"stop\" cell, not part of the trellis)\n",
    "    - bw_trellis\n",
    "    - bw_log_likelihood (the value of the \"start\" cell, not part of the trellis)\n",
    "    \"\"\"\n",
    "    fw_trellis, fw_ll = forward(sequence, p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "    bw_trellis, bw_ll = backward(sequence, p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
    "    return fw_trellis, fw_ll, bw_trellis, bw_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FYVSBx6SiYaf"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12748/2018712751.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mtest_sequence\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest_set\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mfw_trellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfw_ll\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbw_trellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbw_ll\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mforward_backward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12748/1513174515.py\u001B[0m in \u001B[0;36mforward_backward\u001B[1;34m(sequence)\u001B[0m\n\u001B[0;32m     73\u001B[0m     \u001B[1;33m-\u001B[0m \u001B[0mbw_log_likelihood\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mthe\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[1;34m\"start\"\u001B[0m \u001B[0mcell\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mpart\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mtrellis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m     \"\"\"\n\u001B[1;32m---> 75\u001B[1;33m     \u001B[0mfw_trellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfw_ll\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msequence\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_start\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_start\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_trans\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_trans\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_stop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_stop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_emiss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_emiss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m     \u001B[0mbw_trellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbw_ll\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msequence\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_start\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_start\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_trans\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_trans\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_stop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_stop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_emiss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp_emiss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mfw_trellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfw_ll\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbw_trellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbw_ll\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12748/1513174515.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(sequence, p_start, p_trans, p_stop, p_emiss)\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[1;31m# YOUR CODE HERE\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 38\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mNotImplementedError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtrellis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlog_likelihood\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_sequence = test_set[0]\n",
    "\n",
    "fw_trellis, fw_ll, bw_trellis, bw_ll = forward_backward(test_sequence)\n",
    "\n",
    "print(test_sequence)\n",
    "print(fw_trellis)\n",
    "print(fw_ll)\n",
    "print(bw_trellis)\n",
    "print(bw_ll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2KrMiIAiYaf"
   },
   "source": [
    "## Posterior Decoding\n",
    "\n",
    "Implement a function that, given the forward and backward probabilities, and a sequence of observations $o_1, o_2, \\dots, o_N$, returns the best hidden state sequence, according to posterior decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jP_-J3e7iYaf"
   },
   "outputs": [],
   "source": [
    "def posterior_decode(sequence, fw_trellis, bw_trellis, ll, p_trans, p_emiss):\n",
    "    \"\"\"\n",
    "    Return best hidden state sequence according to Posterior decoding\n",
    "    \"\"\"\n",
    "\n",
    "    length = len(sequence)\n",
    "    num_states = fw_trellis.shape[1]\n",
    "        \n",
    "    # calculate the state posteriors\n",
    "    state_posteriors = np.zeros([length, num_states])\n",
    "    \n",
    "    for i in range(length):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    state_posteriors = np.exp(state_posteriors)\n",
    "    \n",
    "    # the best states are simply the arg max of the state posteriors\n",
    "    best_sequence = np.argmax(state_posteriors, axis=1)\n",
    "\n",
    "    return state_posteriors, best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrvFMga1iYag"
   },
   "outputs": [],
   "source": [
    "state_posteriors, best_sequence = posterior_decode(test_sequence, fw_trellis, bw_trellis, fw_ll, p_trans, p_emiss)\n",
    "\n",
    "print(state_posteriors)\n",
    "print(best_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qVlbZcDiYag"
   },
   "source": [
    "              Acknowlegements: A version of this lab was originally developed in collaboration with J. Bastings"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "TempBingoBongoLab2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}